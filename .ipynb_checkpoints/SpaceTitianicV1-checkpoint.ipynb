{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2314,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import adabound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2315,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2316,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2317,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seperated_columns = training_data['Cabin'].str.split('/', expand=True)\n",
    "training_data[['C1', 'C2', 'C3']] = seperated_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2318,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seperated_columns = training_data['PassengerId'].str.split('_', expand=True)\n",
    "training_data[['PI1', 'PI2']] = seperated_columns.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2319,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seperated_columns = training_data['Name'].str.split(' ', expand=True)\n",
    "training_data['Last_name'] = seperated_columns[1]\n",
    "full_training_data = training_data\n",
    "training_data = full_training_data.drop(columns=['PassengerId', 'Cabin', 'Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2320,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Transported', 'C1', 'C2', 'C3', 'PI1', 'PI2', 'Last_name']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>PI1</th>\n",
       "      <th>PI2</th>\n",
       "      <th>Last_name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ofracculy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Vines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Santantines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8688</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>55 Cancri e</td>\n",
       "      <td>41.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1643.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>A</td>\n",
       "      <td>98</td>\n",
       "      <td>P</td>\n",
       "      <td>9276</td>\n",
       "      <td>1</td>\n",
       "      <td>Noxnuther</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8689</th>\n",
       "      <td>Earth</td>\n",
       "      <td>True</td>\n",
       "      <td>PSO J318.5-22</td>\n",
       "      <td>18.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>G</td>\n",
       "      <td>1499</td>\n",
       "      <td>S</td>\n",
       "      <td>9278</td>\n",
       "      <td>1</td>\n",
       "      <td>Mondalley</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8690</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>26.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1872.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>G</td>\n",
       "      <td>1500</td>\n",
       "      <td>S</td>\n",
       "      <td>9279</td>\n",
       "      <td>1</td>\n",
       "      <td>Connon</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8691</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>55 Cancri e</td>\n",
       "      <td>32.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>3235.0</td>\n",
       "      <td>E</td>\n",
       "      <td>608</td>\n",
       "      <td>S</td>\n",
       "      <td>9280</td>\n",
       "      <td>1</td>\n",
       "      <td>Hontichre</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>44.0</td>\n",
       "      <td>False</td>\n",
       "      <td>126.0</td>\n",
       "      <td>4688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>E</td>\n",
       "      <td>608</td>\n",
       "      <td>S</td>\n",
       "      <td>9280</td>\n",
       "      <td>2</td>\n",
       "      <td>Hontichre</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8693 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     HomePlanet CryoSleep    Destination   Age    VIP  RoomService  FoodCourt  \\\n",
       "0        Europa     False    TRAPPIST-1e  39.0  False          0.0        0.0   \n",
       "1         Earth     False    TRAPPIST-1e  24.0  False        109.0        9.0   \n",
       "2        Europa     False    TRAPPIST-1e  58.0   True         43.0     3576.0   \n",
       "3        Europa     False    TRAPPIST-1e  33.0  False          0.0     1283.0   \n",
       "4         Earth     False    TRAPPIST-1e  16.0  False        303.0       70.0   \n",
       "...         ...       ...            ...   ...    ...          ...        ...   \n",
       "8688     Europa     False    55 Cancri e  41.0   True          0.0     6819.0   \n",
       "8689      Earth      True  PSO J318.5-22  18.0  False          0.0        0.0   \n",
       "8690      Earth     False    TRAPPIST-1e  26.0  False          0.0        0.0   \n",
       "8691     Europa     False    55 Cancri e  32.0  False          0.0     1049.0   \n",
       "8692     Europa     False    TRAPPIST-1e  44.0  False        126.0     4688.0   \n",
       "\n",
       "      ShoppingMall     Spa  VRDeck C1    C2 C3   PI1  PI2    Last_name  \\\n",
       "0              0.0     0.0     0.0  B     0  P     1    1    Ofracculy   \n",
       "1             25.0   549.0    44.0  F     0  S     2    1        Vines   \n",
       "2              0.0  6715.0    49.0  A     0  S     3    1       Susent   \n",
       "3            371.0  3329.0   193.0  A     0  S     3    2       Susent   \n",
       "4            151.0   565.0     2.0  F     1  S     4    1  Santantines   \n",
       "...            ...     ...     ... ..   ... ..   ...  ...          ...   \n",
       "8688           0.0  1643.0    74.0  A    98  P  9276    1    Noxnuther   \n",
       "8689           0.0     0.0     0.0  G  1499  S  9278    1    Mondalley   \n",
       "8690        1872.0     1.0     0.0  G  1500  S  9279    1       Connon   \n",
       "8691           0.0   353.0  3235.0  E   608  S  9280    1    Hontichre   \n",
       "8692           0.0     0.0    12.0  E   608  S  9280    2    Hontichre   \n",
       "\n",
       "      Transported  \n",
       "0           False  \n",
       "1            True  \n",
       "2           False  \n",
       "3           False  \n",
       "4            True  \n",
       "...           ...  \n",
       "8688        False  \n",
       "8689        False  \n",
       "8690         True  \n",
       "8691        False  \n",
       "8692         True  \n",
       "\n",
       "[8693 rows x 17 columns]"
      ]
     },
     "execution_count": 2320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = training_data.columns.tolist()\n",
    "print(cols)\n",
    "cols = cols[:10] + cols[11:] + [cols[10]]\n",
    "training_data = training_data[cols]\n",
    "training_data.fillna(0, inplace=True)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2321,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Europa', False, 'TRAPPIST-1e', ..., 1, 'Ofracculy', False],\n",
       "       ['Earth', False, 'TRAPPIST-1e', ..., 1, 'Vines', True],\n",
       "       ['Europa', False, 'TRAPPIST-1e', ..., 1, 'Susent', False],\n",
       "       ...,\n",
       "       ['Earth', False, 'TRAPPIST-1e', ..., 1, 'Connon', True],\n",
       "       ['Europa', False, '55 Cancri e', ..., 1, 'Hontichre', False],\n",
       "       ['Europa', False, 'TRAPPIST-1e', ..., 2, 'Hontichre', True]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = training_data.to_numpy()\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2322,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def class_mapping(column_number):\n",
    "    classes = {}\n",
    "    index = 0\n",
    "    for id in training_data[:, column_number]:\n",
    "        if id not in classes:\n",
    "            classes[id] = index\n",
    "            index += 1\n",
    "    return classes\n",
    "\n",
    "\n",
    "def class_mapping_for_name(column_number):\n",
    "    classes = {0: 0}\n",
    "    index = 1\n",
    "    for id in training_data[:, column_number]:\n",
    "        if id not in classes:\n",
    "            classes[id] = index\n",
    "            index += 1\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2323,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "planet_map = class_mapping(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2324,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{False: 0, True: 1}"
      ]
     },
     "execution_count": 2324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_map = class_mapping(1)\n",
    "TF_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2325,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAPPIST-1e': 0, 'PSO J318.5-22': 1, '55 Cancri e': 2, 0: 3}"
      ]
     },
     "execution_count": 2325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_map = class_mapping(2)\n",
    "destination_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2326,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B': 0, 'F': 1, 'A': 2, 'G': 3, 0: 4, 'E': 5, 'D': 6, 'C': 7, 'T': 8}"
      ]
     },
     "execution_count": 2326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cabin0 = class_mapping(10)\n",
    "cabin0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2327,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P': 0, 'S': 1, 0: 2}"
      ]
     },
     "execution_count": 2327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cabin2 = class_mapping(12)\n",
    "cabin2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2328,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 'Ofracculy': 1,\n",
       " 'Vines': 2,\n",
       " 'Susent': 3,\n",
       " 'Santantines': 4,\n",
       " 'Hinetthews': 5,\n",
       " 'Jacostaffey': 6,\n",
       " 'Beston': 7,\n",
       " 'Flatic': 8,\n",
       " 'Barne': 9,\n",
       " 'Baketton': 10,\n",
       " 'Bertsontry': 11,\n",
       " 'Pooles': 12,\n",
       " 'Eccle': 13,\n",
       " 'Hughriend': 14,\n",
       " 'Upead': 15,\n",
       " 'Brighttt': 16,\n",
       " 'Brantuarez': 17,\n",
       " 'Mcfaddennon': 18,\n",
       " 'Jacostanley': 19,\n",
       " 'Fullided': 20,\n",
       " 'Brookenson': 21,\n",
       " 'Unconary': 22,\n",
       " 'Mare': 23,\n",
       " 'Morsentley': 24,\n",
       " 'Datie': 25,\n",
       " 'Oingwhedly': 26,\n",
       " 'Butte': 27,\n",
       " 'Leodger': 28,\n",
       " 'Wheelez': 29,\n",
       " 'Batthewitt': 30,\n",
       " 'Moodsey': 31,\n",
       " 'Cylistrand': 32,\n",
       " 'Coopelandez': 33,\n",
       " 'Chmad': 34,\n",
       " 'Lancis': 35,\n",
       " 'Johnshines': 36,\n",
       " 'Hubbarton': 37,\n",
       " 'Hickerson': 38,\n",
       " 'Tractive': 39,\n",
       " 'Ayalazquez': 40,\n",
       " 'Salez': 41,\n",
       " 'Greeves': 42,\n",
       " 'Keen': 43,\n",
       " 'Pecketton': 44,\n",
       " 'Leeves': 45,\n",
       " 'Binie': 46,\n",
       " 'Dillines': 47,\n",
       " 'Make': 48,\n",
       " 'Mostedry': 49,\n",
       " 'Dal': 50,\n",
       " 'Blité': 51,\n",
       " 'Pokerheed': 52,\n",
       " 'Vinozarks': 53,\n",
       " 'Datte': 54,\n",
       " 'Coopezmaney': 55,\n",
       " 'Ellcefulve': 56,\n",
       " 'Kinson': 57,\n",
       " 'Sacre': 58,\n",
       " 'Santry': 59,\n",
       " 'Yorkland': 60,\n",
       " 'Dunnisey': 61,\n",
       " 'Hewson': 62,\n",
       " 'Fictful': 63,\n",
       " 'Carezquez': 64,\n",
       " 'Leetersoney': 65,\n",
       " 'Connelson': 66,\n",
       " 'Trad': 67,\n",
       " 'Holson': 68,\n",
       " 'Mclainez': 69,\n",
       " 'Cleachrand': 70,\n",
       " 'Hart': 71,\n",
       " 'Handertiz': 72,\n",
       " 'Aloubtled': 73,\n",
       " 'Bootious': 74,\n",
       " 'Supred': 75,\n",
       " 'Cowtale': 76,\n",
       " 'Carvis': 77,\n",
       " 'Beckerson': 78,\n",
       " 'Pead': 79,\n",
       " 'Suptive': 80,\n",
       " 'Coning': 81,\n",
       " 'Hayder': 82,\n",
       " 'Fielson': 83,\n",
       " 'Mish': 84,\n",
       " 'Mesty': 85,\n",
       " 'Fles': 86,\n",
       " 'Fowles': 87,\n",
       " 'Blie': 88,\n",
       " 'Harmontry': 89,\n",
       " 'Gambs': 90,\n",
       " 'Myling': 91,\n",
       " 'Mejiaddox': 92,\n",
       " 'Whitakers': 93,\n",
       " 'Harte': 94,\n",
       " 'Dayers': 95,\n",
       " 'Wolferguson': 96,\n",
       " 'Hubbarrison': 97,\n",
       " 'Pashe': 98,\n",
       " 'Simson': 99,\n",
       " 'Estron': 100,\n",
       " 'Atkinney': 101,\n",
       " 'Hetforhaft': 102,\n",
       " 'Hutchinton': 103,\n",
       " 'Anche': 104,\n",
       " 'Floaf': 105,\n",
       " 'Fisherry': 106,\n",
       " 'Mcmahoney': 107,\n",
       " 'Gordanieves': 108,\n",
       " 'Grifford': 109,\n",
       " 'Lambles': 110,\n",
       " 'Iniouser': 111,\n",
       " 'Knik': 112,\n",
       " 'Webstephrey': 113,\n",
       " 'Disight': 114,\n",
       " 'Weaves': 115,\n",
       " 'Shephendry': 116,\n",
       " 'Romez': 117,\n",
       " 'Booters': 118,\n",
       " 'Adavisons': 119,\n",
       " 'Raf': 120,\n",
       " 'Emead': 121,\n",
       " 'Dissper': 122,\n",
       " 'Cookson': 123,\n",
       " 'Morrows': 124,\n",
       " 'Colomonson': 125,\n",
       " 'Bacistion': 126,\n",
       " 'Embleng': 127,\n",
       " 'Heateele': 128,\n",
       " 'Oneiles': 129,\n",
       " 'Brugashed': 130,\n",
       " 'Inicont': 131,\n",
       " 'Appie': 132,\n",
       " 'Date': 133,\n",
       " 'Workmans': 134,\n",
       " 'Navages': 135,\n",
       " 'Garnes': 136,\n",
       " 'Shieldson': 137,\n",
       " 'River': 138,\n",
       " 'Wilsoney': 139,\n",
       " 'Blan': 140,\n",
       " 'Reke': 141,\n",
       " 'Perte': 142,\n",
       " 'Deva': 143,\n",
       " 'Sykess': 144,\n",
       " 'Queen': 145,\n",
       " 'Hortis': 146,\n",
       " 'Sellahaney': 147,\n",
       " 'Nate': 148,\n",
       " 'Bakerrison': 149,\n",
       " 'Wist': 150,\n",
       " 'Josey': 151,\n",
       " 'Aginge': 152,\n",
       " 'Resty': 153,\n",
       " 'Crie': 154,\n",
       " 'Dischod': 155,\n",
       " 'Sté': 156,\n",
       " 'Cartyernan': 157,\n",
       " 'Sad': 158,\n",
       " 'Repumparte': 159,\n",
       " 'Sesa': 160,\n",
       " 'Spanxibus': 161,\n",
       " 'Kramosley': 162,\n",
       " 'Amsive': 163,\n",
       " 'Byerry': 164,\n",
       " 'Cardner': 165,\n",
       " 'Stenson': 166,\n",
       " 'Mingious': 167,\n",
       " 'Berte': 168,\n",
       " 'Cooperkins': 169,\n",
       " 'Inderly': 170,\n",
       " 'Bookerson': 171,\n",
       " 'Merkins': 172,\n",
       " 'Herpumble': 173,\n",
       " 'Sancockett': 174,\n",
       " 'Polliamposs': 175,\n",
       " 'Fryersonis': 176,\n",
       " 'Obnoble': 177,\n",
       " 'Mcdanield': 178,\n",
       " 'Beachez': 179,\n",
       " 'Pierry': 180,\n",
       " 'Santin': 181,\n",
       " 'Invebodene': 182,\n",
       " 'Jimes': 183,\n",
       " 'Bootty': 184,\n",
       " 'Delez': 185,\n",
       " 'Objeciane': 186,\n",
       " 'Excialing': 187,\n",
       " 'Brakeril': 188,\n",
       " 'Roforhauge': 189,\n",
       " 'Gardonadox': 190,\n",
       " 'Sorbitter': 191,\n",
       " 'Wriggins': 192,\n",
       " 'Kirklander': 193,\n",
       " 'Ausivetpul': 194,\n",
       " 'Geousker': 195,\n",
       " 'Estevesters': 196,\n",
       " 'Hickett': 197,\n",
       " 'Bowerson': 198,\n",
       " 'Coudered': 199,\n",
       " 'Mckinsond': 200,\n",
       " 'Kochroeders': 201,\n",
       " 'Valezaley': 202,\n",
       " 'Bellarkerd': 203,\n",
       " 'Bache': 204,\n",
       " 'Blange': 205,\n",
       " 'Hooperez': 206,\n",
       " 'Edwartizman': 207,\n",
       " 'Ocherman': 208,\n",
       " 'Siveduced': 209,\n",
       " 'Burchard': 210,\n",
       " 'Melie': 211,\n",
       " 'Mushe': 212,\n",
       " 'Perle': 213,\n",
       " 'Guerson': 214,\n",
       " 'Poille': 215,\n",
       " 'Racke': 216,\n",
       " 'Chanan': 217,\n",
       " 'Kraie': 218,\n",
       " 'Mane': 219,\n",
       " 'Burchanez': 220,\n",
       " 'Strad': 221,\n",
       " 'Warrison': 222,\n",
       " 'Gloyanthy': 223,\n",
       " 'Mcleaney': 224,\n",
       " 'Connon': 225,\n",
       " 'Popez': 226,\n",
       " 'Staffersby': 227,\n",
       " 'Headardyer': 228,\n",
       " 'Jacostaney': 229,\n",
       " 'Pottonalden': 230,\n",
       " 'Coffmaney': 231,\n",
       " 'Dutte': 232,\n",
       " 'Mcneiderson': 233,\n",
       " 'Vincenton': 234,\n",
       " 'Spentley': 235,\n",
       " 'Pirejus': 236,\n",
       " 'Moodson': 237,\n",
       " 'Fuelddid': 238,\n",
       " 'Mosteraked': 239,\n",
       " 'Floydendley': 240,\n",
       " 'Wagnerray': 241,\n",
       " 'Simpsonks': 242,\n",
       " 'Klinson': 243,\n",
       " 'Merce': 244,\n",
       " 'Sullones': 245,\n",
       " 'Savaraldez': 246,\n",
       " 'Cowsearney': 247,\n",
       " 'Carreralend': 248,\n",
       " 'Hervel': 249,\n",
       " 'Robins': 250,\n",
       " 'Mepie': 251,\n",
       " 'Tité': 252,\n",
       " 'Pimne': 253,\n",
       " 'Sageng': 254,\n",
       " 'Wate': 255,\n",
       " 'Flowensley': 256,\n",
       " 'Mirez': 257,\n",
       " 'Ble': 258,\n",
       " 'Iccle': 259,\n",
       " 'Roachoanand': 260,\n",
       " 'Rushorney': 261,\n",
       " 'Quelfly': 262,\n",
       " 'Steinardson': 263,\n",
       " 'Poicaptic': 264,\n",
       " 'Unhearfus': 265,\n",
       " 'Pumbody': 266,\n",
       " 'Fiha': 267,\n",
       " 'Efulows': 268,\n",
       " 'Chocaters': 269,\n",
       " 'Ineysive': 270,\n",
       " 'Jorden': 271,\n",
       " 'Melto': 272,\n",
       " 'Knike': 273,\n",
       " 'Mormonized': 274,\n",
       " 'Pread': 275,\n",
       " 'Willynnedy': 276,\n",
       " 'Hodes': 277,\n",
       " 'Diateous': 278,\n",
       " 'Unciate': 279,\n",
       " 'Hahnstonsen': 280,\n",
       " 'Armstromez': 281,\n",
       " 'Curte': 282,\n",
       " 'Marshopper': 283,\n",
       " 'Singmage': 284,\n",
       " 'Mcclayton': 285,\n",
       " 'Hoppernardy': 286,\n",
       " 'Moongton': 287,\n",
       " 'Ecart': 288,\n",
       " 'Taptiritty': 289,\n",
       " 'Geoconne': 290,\n",
       " 'Fushausive': 291,\n",
       " 'Poperez': 292,\n",
       " 'Bedectical': 293,\n",
       " 'Nichoan': 294,\n",
       " 'Jenningley': 295,\n",
       " 'Sosanturney': 296,\n",
       " 'Staf': 297,\n",
       " 'Chaellerty': 298,\n",
       " 'Wynneyerson': 299,\n",
       " 'Fuelisent': 300,\n",
       " 'Solinez': 301,\n",
       " 'Chardst': 302,\n",
       " 'Gilesonston': 303,\n",
       " 'Chiff': 304,\n",
       " 'Peckerson': 305,\n",
       " 'Mallerez': 306,\n",
       " 'Unloweedic': 307,\n",
       " 'Obeaded': 308,\n",
       " 'Vantonson': 309,\n",
       " 'Preent': 310,\n",
       " 'Meake': 311,\n",
       " 'Cakie': 312,\n",
       " 'Spedlybood': 313,\n",
       " 'Swell': 314,\n",
       " 'Fone': 315,\n",
       " 'Unpasine': 316,\n",
       " 'Backe': 317,\n",
       " 'Mccarry': 318,\n",
       " 'Sinecthex': 319,\n",
       " 'Nateansive': 320,\n",
       " 'Blandez': 321,\n",
       " 'Swinvul': 322,\n",
       " 'Eneurordry': 323,\n",
       " 'Kimons': 324,\n",
       " 'Parta': 325,\n",
       " 'Farleyatton': 326,\n",
       " 'Mcguirez': 327,\n",
       " 'Hornettoney': 328,\n",
       " 'Parklaney': 329,\n",
       " 'Joyneidez': 330,\n",
       " 'Mirezavis': 331,\n",
       " 'Stité': 332,\n",
       " 'Stiven': 333,\n",
       " 'Alentonway': 334,\n",
       " 'Stike': 335,\n",
       " 'Parbage': 336,\n",
       " 'Swanardy': 337,\n",
       " 'Nalanet': 338,\n",
       " 'Conie': 339,\n",
       " 'Ments': 340,\n",
       " 'Sextones': 341,\n",
       " 'Weavesend': 342,\n",
       " 'Webstes': 343,\n",
       " 'Watte': 344,\n",
       " 'Cleminglas': 345,\n",
       " 'Bradlerson': 346,\n",
       " 'Brathful': 347,\n",
       " 'Valindle': 348,\n",
       " 'Graciansus': 349,\n",
       " 'Rames': 350,\n",
       " 'Harmond': 351,\n",
       " 'Hortez': 352,\n",
       " 'Flemaney': 353,\n",
       " 'Petie': 354,\n",
       " 'Dinsprody': 355,\n",
       " 'Munozanson': 356,\n",
       " 'Baciffhaut': 357,\n",
       " 'Tramsor': 358,\n",
       " 'Ousious': 359,\n",
       " 'Hednigic': 360,\n",
       " 'Lamberguson': 361,\n",
       " 'Dickley': 362,\n",
       " 'Herry': 363,\n",
       " 'Trankliney': 364,\n",
       " 'Woodgezalez': 365,\n",
       " 'Chort': 366,\n",
       " 'Erle': 367,\n",
       " 'Oillicaly': 368,\n",
       " 'Tecre': 369,\n",
       " 'Delazarson': 370,\n",
       " 'Hardley': 371,\n",
       " 'Kra': 372,\n",
       " 'Flynner': 373,\n",
       " 'Gene': 374,\n",
       " 'Buckeelez': 375,\n",
       " 'Crité': 376,\n",
       " 'Ormler': 377,\n",
       " 'Brine': 378,\n",
       " 'Scheron': 379,\n",
       " 'Hontichre': 380,\n",
       " 'Rilley': 381,\n",
       " 'Sacle': 382,\n",
       " 'Potters': 383,\n",
       " 'Ranaly': 384,\n",
       " 'Carterson': 385,\n",
       " 'Pollarkeeks': 386,\n",
       " 'Holatirint': 387,\n",
       " 'Gaington': 388,\n",
       " 'Gainebergan': 389,\n",
       " 'Portananney': 390,\n",
       " 'Anpie': 391,\n",
       " 'She': 392,\n",
       " 'Gread': 393,\n",
       " 'Wolfaddox': 394,\n",
       " 'Powery': 395,\n",
       " 'Pamca': 396,\n",
       " 'Wooterston': 397,\n",
       " 'Yanton': 398,\n",
       " 'Kiling': 399,\n",
       " 'Watie': 400,\n",
       " 'Prucerod': 401,\n",
       " 'Grahangory': 402,\n",
       " 'Trattle': 403,\n",
       " 'Deeke': 404,\n",
       " 'Hammonton': 405,\n",
       " 'Paska': 406,\n",
       " 'Cure': 407,\n",
       " 'Gordond': 408,\n",
       " 'Jarvey': 409,\n",
       " 'Andackson': 410,\n",
       " 'Saley': 411,\n",
       " 'Ticagent': 412,\n",
       " 'Buckentry': 413,\n",
       " 'Winie': 414,\n",
       " 'Torrez': 415,\n",
       " 'Hardson': 416,\n",
       " 'Dillips': 417,\n",
       " 'Moodman': 418,\n",
       " 'Camerrison': 419,\n",
       " 'Cla': 420,\n",
       " 'Parté': 421,\n",
       " 'Lerez': 422,\n",
       " 'Bert': 423,\n",
       " 'Concy': 424,\n",
       " 'Erpie': 425,\n",
       " 'Motive': 426,\n",
       " 'Start': 427,\n",
       " 'Potte': 428,\n",
       " 'Dedometeel': 429,\n",
       " 'Haydenzier': 430,\n",
       " 'Buchansen': 431,\n",
       " 'Sha': 432,\n",
       " 'Rhuba': 433,\n",
       " 'Voliat': 434,\n",
       " 'Estein': 435,\n",
       " 'Wartyson': 436,\n",
       " 'Mccarveymon': 437,\n",
       " 'Missefle': 438,\n",
       " 'Coflé': 439,\n",
       " 'Apeau': 440,\n",
       " 'Stantiagord': 441,\n",
       " 'Mondsey': 442,\n",
       " 'Reate': 443,\n",
       " 'Bensley': 444,\n",
       " 'Fitzgerson': 445,\n",
       " 'Potthews': 446,\n",
       " 'Currisones': 447,\n",
       " 'Lewinton': 448,\n",
       " 'Bairdford': 449,\n",
       " 'Hamberterry': 450,\n",
       " 'Ewiseston': 451,\n",
       " 'Mcbritts': 452,\n",
       " 'Inghtal': 453,\n",
       " 'Waltonnedy': 454,\n",
       " 'Ennalve': 455,\n",
       " 'Fusimodent': 456,\n",
       " 'Ainserfle': 457,\n",
       " 'Geraing': 458,\n",
       " 'Traie': 459,\n",
       " 'Fliblerolt': 460,\n",
       " 'Toddleton': 461,\n",
       " 'Burle': 462,\n",
       " 'Meltonway': 463,\n",
       " 'Oilloody': 464,\n",
       " 'Reeddommy': 465,\n",
       " 'Mclardson': 466,\n",
       " 'Steeletters': 467,\n",
       " 'Plegred': 468,\n",
       " 'Roycentes': 469,\n",
       " 'Plattering': 470,\n",
       " 'Rodger': 471,\n",
       " 'Valexandez': 472,\n",
       " 'Walé': 473,\n",
       " 'Aiming': 474,\n",
       " 'Hartz': 475,\n",
       " 'Quinnerry': 476,\n",
       " 'Imprank': 477,\n",
       " 'Frolestty': 478,\n",
       " 'Pottmanley': 479,\n",
       " 'Mcfarmerson': 480,\n",
       " 'Powenoble': 481,\n",
       " 'Klindsey': 482,\n",
       " 'Unfile': 483,\n",
       " 'Noelan': 484,\n",
       " 'Gelle': 485,\n",
       " 'Presstic': 486,\n",
       " 'Chuble': 487,\n",
       " 'Nieldson': 488,\n",
       " 'Ramoran': 489,\n",
       " 'Nutte': 490,\n",
       " 'Katte': 491,\n",
       " 'Meyersones': 492,\n",
       " 'Asoppor': 493,\n",
       " 'Baxters': 494,\n",
       " 'Couseced': 495,\n",
       " 'Gainney': 496,\n",
       " 'Strongers': 497,\n",
       " 'Sweeton': 498,\n",
       " 'Carrishley': 499,\n",
       " 'Chpie': 500,\n",
       " 'Wardendez': 501,\n",
       " 'Hoopez': 502,\n",
       " 'Carverson': 503,\n",
       " 'Hinesh': 504,\n",
       " 'Pacerty': 505,\n",
       " 'Frazie': 506,\n",
       " 'Braymon': 507,\n",
       " 'Fueling': 508,\n",
       " 'Simstravery': 509,\n",
       " 'Raca': 510,\n",
       " 'Oidederval': 511,\n",
       " 'Nan': 512,\n",
       " 'Sarios': 513,\n",
       " 'Ageurante': 514,\n",
       " 'Walkerez': 515,\n",
       " 'Sandul': 516,\n",
       " 'Stephendsey': 517,\n",
       " 'Peckers': 518,\n",
       " 'Sweene': 519,\n",
       " 'Maynardner': 520,\n",
       " 'Geneltic': 521,\n",
       " 'Franton': 522,\n",
       " 'Briggston': 523,\n",
       " 'Covetive': 524,\n",
       " 'Bancy': 525,\n",
       " 'Lanks': 526,\n",
       " 'Reedall': 527,\n",
       " 'Malliamsey': 528,\n",
       " 'Pittson': 529,\n",
       " 'Polksy': 530,\n",
       " 'Barris': 531,\n",
       " 'Alldson': 532,\n",
       " 'Warrishales': 533,\n",
       " 'Porki': 534,\n",
       " 'Alcemblery': 535,\n",
       " 'Kin': 536,\n",
       " 'Cassiding': 537,\n",
       " 'Burryerson': 538,\n",
       " 'Blate': 539,\n",
       " 'Parymider': 540,\n",
       " 'Fuenterson': 541,\n",
       " 'Vinston': 542,\n",
       " 'Obedaming': 543,\n",
       " 'Stiviorad': 544,\n",
       " 'Asticit': 545,\n",
       " 'Avisnydes': 546,\n",
       " 'Ryanglasey': 547,\n",
       " 'Sweett': 548,\n",
       " 'Flynnis': 549,\n",
       " 'Pette': 550,\n",
       " 'Fawnsive': 551,\n",
       " 'Rocknight': 552,\n",
       " 'Peter': 553,\n",
       " 'Byerton': 554,\n",
       " 'Vloaf': 555,\n",
       " 'Weaverays': 556,\n",
       " 'Emenez': 557,\n",
       " 'Frandriquez': 558,\n",
       " 'Johnsby': 559,\n",
       " 'Lunapperts': 560,\n",
       " 'Colle': 561,\n",
       " 'Smalloney': 562,\n",
       " 'Bartez': 563,\n",
       " 'Hapie': 564,\n",
       " 'Youngrayes': 565,\n",
       " 'Lewinez': 566,\n",
       " 'Platch': 567,\n",
       " 'Depie': 568,\n",
       " 'Fulloydez': 569,\n",
       " 'Figuez': 570,\n",
       " 'Emone': 571,\n",
       " 'Blace': 572,\n",
       " 'Humphreyes': 573,\n",
       " 'Coxterez': 574,\n",
       " 'Wiggs': 575,\n",
       " 'Woodwin': 576,\n",
       " 'Sweekstarks': 577,\n",
       " 'Reyerson': 578,\n",
       " 'Norrison': 579,\n",
       " 'Moranthons': 580,\n",
       " 'Cochrisons': 581,\n",
       " 'Acobsond': 582,\n",
       " 'Miltongson': 583,\n",
       " 'Barmant': 584,\n",
       " 'Shpie': 585,\n",
       " 'Suptibler': 586,\n",
       " 'Llonovancis': 587,\n",
       " 'Leste': 588,\n",
       " 'Wassird': 589,\n",
       " 'Juartis': 590,\n",
       " 'Summington': 591,\n",
       " 'Spistory': 592,\n",
       " 'Prinson': 593,\n",
       " 'Frasp': 594,\n",
       " 'Antoshipson': 595,\n",
       " 'Blanglison': 596,\n",
       " 'Wilderssen': 597,\n",
       " 'Wala': 598,\n",
       " 'Chavezalez': 599,\n",
       " 'Nichaner': 600,\n",
       " 'Harverez': 601,\n",
       " 'Leonzaley': 602,\n",
       " 'Garzaley': 603,\n",
       " 'Gonzaley': 604,\n",
       " 'Cleman': 605,\n",
       " 'Bra': 606,\n",
       " 'Georgasey': 607,\n",
       " 'Knoxonway': 608,\n",
       " 'Heedry': 609,\n",
       " 'Talls': 610,\n",
       " 'Unteckery': 611,\n",
       " 'Mcdon': 612,\n",
       " 'Prie': 613,\n",
       " 'Gallard': 614,\n",
       " 'Colleruces': 615,\n",
       " 'Braned': 616,\n",
       " 'Ingston': 617,\n",
       " 'Clayson': 618,\n",
       " 'Wolffy': 619,\n",
       " 'Ingwhed': 620,\n",
       " 'Jaf': 621,\n",
       " 'Pughanders': 622,\n",
       " 'Dingauge': 623,\n",
       " 'Bardinard': 624,\n",
       " 'Ewins': 625,\n",
       " 'Traverdy': 626,\n",
       " 'Parte': 627,\n",
       " 'Sfin': 628,\n",
       " 'Holcompson': 629,\n",
       " 'Carpennels': 630,\n",
       " 'Willangsey': 631,\n",
       " 'Pottney': 632,\n",
       " 'Pronfraked': 633,\n",
       " 'Miste': 634,\n",
       " 'Webstenson': 635,\n",
       " 'Drelcate': 636,\n",
       " 'Weeke': 637,\n",
       " 'Mcneiley': 638,\n",
       " 'Cuche': 639,\n",
       " 'Frité': 640,\n",
       " 'Dan': 641,\n",
       " 'Seen': 642,\n",
       " 'Opshaft': 643,\n",
       " 'Fielsenders': 644,\n",
       " 'Kidne': 645,\n",
       " 'Oneidson': 646,\n",
       " 'Barnolaney': 647,\n",
       " 'Burnerez': 648,\n",
       " 'Praded': 649,\n",
       " 'Dishocatal': 650,\n",
       " 'Bellyonsley': 651,\n",
       " 'Cako': 652,\n",
       " 'Benney': 653,\n",
       " 'Lake': 654,\n",
       " 'Sobvisted': 655,\n",
       " 'Bowenasey': 656,\n",
       " 'Sofraten': 657,\n",
       " 'Pugherman': 658,\n",
       " 'Barnolderg': 659,\n",
       " 'Roses': 660,\n",
       " 'Semageary': 661,\n",
       " 'Clugete': 662,\n",
       " 'Logannon': 663,\n",
       " 'Quinnerettt': 664,\n",
       " 'Vaught': 665,\n",
       " 'Brookes': 666,\n",
       " 'Corman': 667,\n",
       " 'Ingascomet': 668,\n",
       " 'Hessey': 669,\n",
       " 'Motter': 670,\n",
       " 'Anderking': 671,\n",
       " 'Radisiouss': 672,\n",
       " 'Heermy': 673,\n",
       " 'Waring': 674,\n",
       " 'Shoeweely': 675,\n",
       " 'Terry': 676,\n",
       " 'Sin': 677,\n",
       " 'Gallandez': 678,\n",
       " 'Lessonerry': 679,\n",
       " 'Brin': 680,\n",
       " 'Wasterivel': 681,\n",
       " 'Irwines': 682,\n",
       " 'Pota': 683,\n",
       " 'Nutty': 684,\n",
       " 'Browlerson': 685,\n",
       " 'Matts': 686,\n",
       " 'Sorticbar': 687,\n",
       " 'Folhal': 688,\n",
       " 'Tie': 689,\n",
       " 'Hubbarnett': 690,\n",
       " 'Unrent': 691,\n",
       " 'Blainson': 692,\n",
       " 'Brooker': 693,\n",
       " 'Alvasquez': 694,\n",
       " 'Vales': 695,\n",
       " 'Huntt': 696,\n",
       " 'Pentss': 697,\n",
       " 'Gilleranks': 698,\n",
       " 'Fanchy': 699,\n",
       " 'Alfordonard': 700,\n",
       " 'Hubert': 701,\n",
       " 'Coweboded': 702,\n",
       " 'Morrodgers': 703,\n",
       " 'Purle': 704,\n",
       " 'Scottuez': 705,\n",
       " 'Lancock': 706,\n",
       " 'Bookson': 707,\n",
       " 'Clugeadry': 708,\n",
       " 'Drivery': 709,\n",
       " 'Oderient': 710,\n",
       " 'Schmondez': 711,\n",
       " 'Breke': 712,\n",
       " 'Scotterkins': 713,\n",
       " 'Healted': 714,\n",
       " 'Roberrenn': 715,\n",
       " 'Pane': 716,\n",
       " 'Cruzaley': 717,\n",
       " 'Howayery': 718,\n",
       " 'Dynigic': 719,\n",
       " 'Leton': 720,\n",
       " 'Tranciams': 721,\n",
       " 'Knité': 722,\n",
       " 'Fache': 723,\n",
       " 'Fultz': 724,\n",
       " 'Ponie': 725,\n",
       " 'Conale': 726,\n",
       " 'Gulcepusy': 727,\n",
       " 'Erry': 728,\n",
       " 'Quinnedy': 729,\n",
       " 'Irintious': 730,\n",
       " 'Coillery': 731,\n",
       " 'Masquez': 732,\n",
       " 'Gamberson': 733,\n",
       " 'Gilleyons': 734,\n",
       " 'Barbes': 735,\n",
       " 'Gelte': 736,\n",
       " 'Rigginsen': 737,\n",
       " 'Datty': 738,\n",
       " 'Alshipson': 739,\n",
       " 'Hurchez': 740,\n",
       " 'Fryes': 741,\n",
       " 'Shermann': 742,\n",
       " 'Harrenoldez': 743,\n",
       " 'Mayder': 744,\n",
       " 'Horners': 745,\n",
       " 'Balightal': 746,\n",
       " 'Cleang': 747,\n",
       " 'Gerry': 748,\n",
       " 'Tepie': 749,\n",
       " 'Paynard': 750,\n",
       " 'Rothewson': 751,\n",
       " 'Stre': 752,\n",
       " 'Exnutch': 753,\n",
       " 'Hestes': 754,\n",
       " 'Waterson': 755,\n",
       " 'Kuche': 756,\n",
       " 'Coniple': 757,\n",
       " 'Caineson': 758,\n",
       " 'Mcgowaymond': 759,\n",
       " 'Fuleng': 760,\n",
       " 'Daughttley': 761,\n",
       " 'Resendent': 762,\n",
       " 'Mayods': 763,\n",
       " 'Foxter': 764,\n",
       " 'Hacle': 765,\n",
       " 'Lowelliott': 766,\n",
       " 'Meltoney': 767,\n",
       " 'Apie': 768,\n",
       " 'Kake': 769,\n",
       " 'Cookentaney': 770,\n",
       " 'Popelase': 771,\n",
       " 'Burtiz': 772,\n",
       " 'Blacks': 773,\n",
       " 'Mornaned': 774,\n",
       " 'Gres': 775,\n",
       " 'Winsley': 776,\n",
       " 'Higast': 777,\n",
       " 'Macdonadows': 778,\n",
       " 'Spedlinale': 779,\n",
       " 'Shbre': 780,\n",
       " 'Cres': 781,\n",
       " 'Rodwative': 782,\n",
       " 'Hanghanson': 783,\n",
       " 'Noeley': 784,\n",
       " 'Cartez': 785,\n",
       " 'Sancasey': 786,\n",
       " 'Lette': 787,\n",
       " 'Coarswing': 788,\n",
       " 'Wellenez': 789,\n",
       " 'Ailled': 790,\n",
       " 'Proorbeng': 791,\n",
       " 'Distured': 792,\n",
       " 'Thorney': 793,\n",
       " 'Righturter': 794,\n",
       " 'Conaly': 795,\n",
       " 'Combson': 796,\n",
       " 'Williotters': 797,\n",
       " 'Melte': 798,\n",
       " 'Prist': 799,\n",
       " 'Man': 800,\n",
       " 'Betie': 801,\n",
       " 'Gallencis': 802,\n",
       " 'Crioncery': 803,\n",
       " 'Sosalinson': 804,\n",
       " 'Keithunts': 805,\n",
       " 'Fryan': 806,\n",
       " 'Haste': 807,\n",
       " 'Jeffey': 808,\n",
       " 'Swin': 809,\n",
       " 'Spandisket': 810,\n",
       " 'Whealeed': 811,\n",
       " 'Oconley': 812,\n",
       " 'Flité': 813,\n",
       " 'Shawforden': 814,\n",
       " 'Gin': 815,\n",
       " 'Sotond': 816,\n",
       " 'Boderbint': 817,\n",
       " 'Madan': 818,\n",
       " 'Slable': 819,\n",
       " 'Puckley': 820,\n",
       " 'Bealvewd': 821,\n",
       " 'Nuthe': 822,\n",
       " 'Clumshangs': 823,\n",
       " 'Woody': 824,\n",
       " 'Seche': 825,\n",
       " 'Clidly': 826,\n",
       " 'Hoppers': 827,\n",
       " 'Unicting': 828,\n",
       " 'Stivelte': 829,\n",
       " 'Supiensive': 830,\n",
       " 'Batestonley': 831,\n",
       " 'Mcdowns': 832,\n",
       " 'Guezaley': 833,\n",
       " 'Briggins': 834,\n",
       " 'Simmonders': 835,\n",
       " 'Merie': 836,\n",
       " 'Replic': 837,\n",
       " 'Burchrison': 838,\n",
       " 'Hac': 839,\n",
       " 'Pager': 840,\n",
       " 'Garrincent': 841,\n",
       " 'Che': 842,\n",
       " 'Thony': 843,\n",
       " 'Crosarios': 844,\n",
       " 'Fullous': 845,\n",
       " 'Crakenecal': 846,\n",
       " 'Idivent': 847,\n",
       " 'Reming': 848,\n",
       " 'Replipent': 849,\n",
       " 'Mulley': 850,\n",
       " 'Chardsons': 851,\n",
       " 'Rostarks': 852,\n",
       " 'Dicksons': 853,\n",
       " 'Entenedy': 854,\n",
       " 'Lopelases': 855,\n",
       " 'Harperez': 856,\n",
       " 'Hinglendez': 857,\n",
       " 'Dukertyler': 858,\n",
       " 'Sau': 859,\n",
       " 'Patimonne': 860,\n",
       " 'Foraud': 861,\n",
       " 'Cofferson': 862,\n",
       " 'Kennetty': 863,\n",
       " 'Fatted': 864,\n",
       " 'Conleydenan': 865,\n",
       " 'Tuckers': 866,\n",
       " 'Ricews': 867,\n",
       " 'Ballocks': 868,\n",
       " 'Thonyderson': 869,\n",
       " 'Monroellynn': 870,\n",
       " 'Fisheparks': 871,\n",
       " 'Wonglasquez': 872,\n",
       " 'Lynnerett': 873,\n",
       " 'Alest': 874,\n",
       " 'Part': 875,\n",
       " 'Fringh': 876,\n",
       " 'Earmeter': 877,\n",
       " 'Sterson': 878,\n",
       " 'Villenson': 879,\n",
       " 'Retefule': 880,\n",
       " 'Watsongley': 881,\n",
       " 'Spane': 882,\n",
       " 'Solivers': 883,\n",
       " 'Perrerojass': 884,\n",
       " 'Englence': 885,\n",
       " 'Dillestron': 886,\n",
       " 'Batte': 887,\n",
       " 'Paulllegory': 888,\n",
       " 'Giba': 889,\n",
       " 'Moraleney': 890,\n",
       " 'Iderinelay': 891,\n",
       " 'Barkeretton': 892,\n",
       " 'Fincy': 893,\n",
       " 'Chake': 894,\n",
       " 'Hopez': 895,\n",
       " 'Minen': 896,\n",
       " 'Dyers': 897,\n",
       " 'Parsoney': 898,\n",
       " 'Mirandry': 899,\n",
       " 'Franankson': 900,\n",
       " 'Reyervaney': 901,\n",
       " 'Barte': 902,\n",
       " 'Bakerson': 903,\n",
       " 'Curton': 904,\n",
       " 'Patrichane': 905,\n",
       " 'Blancoy': 906,\n",
       " 'Sawyerson': 907,\n",
       " 'Coley': 908,\n",
       " 'Bradamss': 909,\n",
       " 'Cardsay': 910,\n",
       " 'Casez': 911,\n",
       " 'Stering': 912,\n",
       " 'Laai': 913,\n",
       " 'Gleroling': 914,\n",
       " 'Stewder': 915,\n",
       " 'Beckeey': 916,\n",
       " 'Sinda': 917,\n",
       " 'Wincer': 918,\n",
       " 'Butty': 919,\n",
       " 'Canie': 920,\n",
       " 'Adkinson': 921,\n",
       " 'Hermanapp': 922,\n",
       " 'Brailidy': 923,\n",
       " 'Garnettiz': 924,\n",
       " 'Kniva': 925,\n",
       " 'Brazo': 926,\n",
       " 'Hariourcal': 927,\n",
       " 'Blanspe': 928,\n",
       " 'Mcfarley': 929,\n",
       " 'Leonaldez': 930,\n",
       " 'Goffey': 931,\n",
       " 'Curle': 932,\n",
       " 'Hotte': 933,\n",
       " 'Ingetrody': 934,\n",
       " 'Ruité': 935,\n",
       " 'Hickson': 936,\n",
       " 'Cainghannon': 937,\n",
       " 'Dicomeng': 938,\n",
       " 'Tertchty': 939,\n",
       " 'Cohendright': 940,\n",
       " 'Byerson': 941,\n",
       " 'Lewintos': 942,\n",
       " 'Hultz': 943,\n",
       " 'Gloother': 944,\n",
       " 'Persed': 945,\n",
       " 'Scoudelle': 946,\n",
       " 'Meleon': 947,\n",
       " 'Flynney': 948,\n",
       " 'Unaasor': 949,\n",
       " 'Stannondez': 950,\n",
       " 'Mejiamsey': 951,\n",
       " 'Sellentry': 952,\n",
       " 'Schwardson': 953,\n",
       " 'Unprive': 954,\n",
       " 'Jenson': 955,\n",
       " 'Headfair': 956,\n",
       " 'Clake': 957,\n",
       " 'Erflé': 958,\n",
       " 'Owery': 959,\n",
       " 'Barrison': 960,\n",
       " 'Ellahan': 961,\n",
       " 'Chenson': 962,\n",
       " 'Clean': 963,\n",
       " 'Valey': 964,\n",
       " 'Antcal': 965,\n",
       " 'Pooley': 966,\n",
       " 'Barrelliver': 967,\n",
       " 'Stpingepid': 968,\n",
       " 'Gocent': 969,\n",
       " 'Fishopper': 970,\n",
       " 'Loway': 971,\n",
       " 'Booneid': 972,\n",
       " 'Neceriont': 973,\n",
       " 'Chcak': 974,\n",
       " 'Castery': 975,\n",
       " 'Assibler': 976,\n",
       " 'Horte': 977,\n",
       " 'Dister': 978,\n",
       " 'Conney': 979,\n",
       " 'Anthon': 980,\n",
       " 'Dressink': 981,\n",
       " 'Hammons': 982,\n",
       " 'Gallenry': 983,\n",
       " 'Knoxtones': 984,\n",
       " 'Charpelaney': 985,\n",
       " 'Burta': 986,\n",
       " 'Bate': 987,\n",
       " 'Hendrewsons': 988,\n",
       " 'Gaineyerson': 989,\n",
       " 'Castannison': 990,\n",
       " 'Vandan': 991,\n",
       " 'Welles': 992,\n",
       " 'Exibuctich': 993,\n",
       " 'Giller': 994,\n",
       " 'Buckeenez': 995,\n",
       " 'Ryanga': 996,\n",
       " 'Fowlaney': 997,\n",
       " 'Irwinson': 998,\n",
       " 'Clindy': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 2328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_name_map = class_mapping_for_name(15)\n",
    "print(len(last_name_map))\n",
    "last_name_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2329,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def replace_with_mapped_value(column_number, map_name, data_set):\n",
    "    data_set[:, column_number] = np.vectorize(map_name.get)(data_set[:, column_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2330,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, False, 'TRAPPIST-1e', ..., 1, 'Ofracculy', False],\n",
       "       [1, False, 'TRAPPIST-1e', ..., 1, 'Vines', True],\n",
       "       [0, False, 'TRAPPIST-1e', ..., 1, 'Susent', False],\n",
       "       ...,\n",
       "       [1, False, 'TRAPPIST-1e', ..., 1, 'Connon', True],\n",
       "       [0, False, '55 Cancri e', ..., 1, 'Hontichre', False],\n",
       "       [0, False, 'TRAPPIST-1e', ..., 2, 'Hontichre', True]], dtype=object)"
      ]
     },
     "execution_count": 2330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_with_mapped_value(0, planet_map, training_data)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2331,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "replace_with_mapped_value(1, TF_map, training_data)\n",
    "replace_with_mapped_value(2, destination_map, training_data)\n",
    "replace_with_mapped_value(4, TF_map, training_data)\n",
    "replace_with_mapped_value(10, cabin0, training_data)\n",
    "replace_with_mapped_value(12, cabin2, training_data)\n",
    "replace_with_mapped_value(15, last_name_map, training_data)\n",
    "replace_with_mapped_value(16, TF_map, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2332,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 1, 0],\n",
       "       [1, 0, 0, ..., 1, 2, 1],\n",
       "       [0, 0, 0, ..., 1, 3, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 1, 225, 1],\n",
       "       [0, 0, 2, ..., 1, 380, 0],\n",
       "       [0, 0, 0, ..., 2, 380, 1]], dtype=object)"
      ]
     },
     "execution_count": 2332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2333,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# np.random.shuffle(training_data)\n",
    "# training_data, test_data = training_data[:int(len(training_data) * 0.8)], training_data[int(len(training_data) * 0.8):]\n",
    "np.random.shuffle(training_data)\n",
    "training_data, test_data = training_data, training_data\n",
    "\n",
    "\n",
    "# x=np.delete(training_data,[0,1,2],1).astype(float)\n",
    "# y=training_data[:,[1]].astype(float)\n",
    "# tensor_x=torch.Tensor(x)\n",
    "# tensor_y=torch.Tensor(y)\n",
    "# training_data=TensorDataset(tensor_x,tensor_y)\n",
    "def to_tensor(np_array, target_column):\n",
    "    x = np.delete(np_array, target_column, axis=1).astype(float)\n",
    "    y = np_array[:, target_column].astype(int)\n",
    "    y_with_zeros = []\n",
    "    for class_element in y:\n",
    "        y_with_zeros.append(np.zeros(len(TF_map)))\n",
    "        y_with_zeros[-1][class_element] = 1\n",
    "    y = y_with_zeros\n",
    "    tensor_x = torch.Tensor(x)\n",
    "    tensor_y = torch.Tensor(y)\n",
    "    tensor = TensorDataset(tensor_x, tensor_y)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2334,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16])\n",
      "torch.Size([128, 2]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(to_tensor(training_data, 16), batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(to_tensor(test_data, 16), batch_size=batch_size)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(X.shape)\n",
    "    print(y.shape, y.dtype)\n",
    "    break\n",
    "\n",
    "# # Display sample data\n",
    "# figure = plt.figure(figsize=(10, 8))\n",
    "# cols, rows = 5, 5\n",
    "# for i in range(1, cols * rows + 1):\n",
    "#     idx = torch.randint(len(test_data), size=(1,)).item()\n",
    "#     img, label = test_data[idx]\n",
    "#     figure.add_subplot(rows, cols, i)\n",
    "#     plt.title(label)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2335,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (35): ReLU()\n",
      "    (36): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (37): ReLU()\n",
      "    (38): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (39): ReLU()\n",
      "    (40): Linear(in_features=1024, out_features=2, bias=True)\n",
      "    (41): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "layer_size = 512\n",
    "input_size = 16\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_size, len(TF_map)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2336,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss_fn = nn.L1Loss()\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 1e-3\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = adabound.AdaBound(model.parameters(), lr=learning_rate,final_lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2337,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # y=y.type(torch.float)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 8 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2338,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    try:\n",
    "        if correct > 0.82:\n",
    "            torch.save(model, 'model/m3')\n",
    "        elif correct > 0.815:\n",
    "            torch.save(model, 'model/m2')\n",
    "        elif correct > 0.81:\n",
    "            torch.save(model, 'model/m1')\n",
    "        elif correct > 0.805:\n",
    "            torch.save(model, 'model/m0')\n",
    "        elif correct > 0.8:\n",
    "            torch.save(model, 'model/ml')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2339,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.250475  [    0/ 8693]\n",
      "loss: 0.250505  [ 1024/ 8693]\n",
      "loss: 0.249454  [ 2048/ 8693]\n",
      "loss: 0.249769  [ 3072/ 8693]\n",
      "loss: 0.249847  [ 4096/ 8693]\n",
      "loss: 0.250049  [ 5120/ 8693]\n",
      "loss: 0.250921  [ 6144/ 8693]\n",
      "loss: 0.249851  [ 7168/ 8693]\n",
      "loss: 0.249997  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 0.001956 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.249998  [    0/ 8693]\n",
      "loss: 0.250104  [ 1024/ 8693]\n",
      "loss: 0.250052  [ 2048/ 8693]\n",
      "loss: 0.250056  [ 3072/ 8693]\n",
      "loss: 0.250213  [ 4096/ 8693]\n",
      "loss: 0.250230  [ 5120/ 8693]\n",
      "loss: 0.250203  [ 6144/ 8693]\n",
      "loss: 0.249912  [ 7168/ 8693]\n",
      "loss: 0.249813  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001956 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.249727  [    0/ 8693]\n",
      "loss: 0.250137  [ 1024/ 8693]\n",
      "loss: 0.250490  [ 2048/ 8693]\n",
      "loss: 0.249862  [ 3072/ 8693]\n",
      "loss: 0.249984  [ 4096/ 8693]\n",
      "loss: 0.249940  [ 5120/ 8693]\n",
      "loss: 0.249989  [ 6144/ 8693]\n",
      "loss: 0.249808  [ 7168/ 8693]\n",
      "loss: 0.250192  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001956 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.249585  [    0/ 8693]\n",
      "loss: 0.250449  [ 1024/ 8693]\n",
      "loss: 0.250087  [ 2048/ 8693]\n",
      "loss: 0.250065  [ 3072/ 8693]\n",
      "loss: 0.250056  [ 4096/ 8693]\n",
      "loss: 0.249504  [ 5120/ 8693]\n",
      "loss: 0.249851  [ 6144/ 8693]\n",
      "loss: 0.249830  [ 7168/ 8693]\n",
      "loss: 0.249685  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.250483  [    0/ 8693]\n",
      "loss: 0.250123  [ 1024/ 8693]\n",
      "loss: 0.249847  [ 2048/ 8693]\n",
      "loss: 0.250026  [ 3072/ 8693]\n",
      "loss: 0.250771  [ 4096/ 8693]\n",
      "loss: 0.248574  [ 5120/ 8693]\n",
      "loss: 0.250213  [ 6144/ 8693]\n",
      "loss: 0.250093  [ 7168/ 8693]\n",
      "loss: 0.250695  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.249386  [    0/ 8693]\n",
      "loss: 0.249374  [ 1024/ 8693]\n",
      "loss: 0.251219  [ 2048/ 8693]\n",
      "loss: 0.249212  [ 3072/ 8693]\n",
      "loss: 0.250030  [ 4096/ 8693]\n",
      "loss: 0.249950  [ 5120/ 8693]\n",
      "loss: 0.249905  [ 6144/ 8693]\n",
      "loss: 0.249950  [ 7168/ 8693]\n",
      "loss: 0.250218  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.250355  [    0/ 8693]\n",
      "loss: 0.250764  [ 1024/ 8693]\n",
      "loss: 0.250169  [ 2048/ 8693]\n",
      "loss: 0.249864  [ 3072/ 8693]\n",
      "loss: 0.250125  [ 4096/ 8693]\n",
      "loss: 0.250042  [ 5120/ 8693]\n",
      "loss: 0.249831  [ 6144/ 8693]\n",
      "loss: 0.250150  [ 7168/ 8693]\n",
      "loss: 0.249793  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.250014  [    0/ 8693]\n",
      "loss: 0.250011  [ 1024/ 8693]\n",
      "loss: 0.249960  [ 2048/ 8693]\n",
      "loss: 0.249656  [ 3072/ 8693]\n",
      "loss: 0.249660  [ 4096/ 8693]\n",
      "loss: 0.250080  [ 5120/ 8693]\n",
      "loss: 0.249618  [ 6144/ 8693]\n",
      "loss: 0.250794  [ 7168/ 8693]\n",
      "loss: 0.250267  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001956 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.250552  [    0/ 8693]\n",
      "loss: 0.249719  [ 1024/ 8693]\n",
      "loss: 0.250627  [ 2048/ 8693]\n",
      "loss: 0.250989  [ 3072/ 8693]\n",
      "loss: 0.250072  [ 4096/ 8693]\n",
      "loss: 0.249946  [ 5120/ 8693]\n",
      "loss: 0.250115  [ 6144/ 8693]\n",
      "loss: 0.249915  [ 7168/ 8693]\n",
      "loss: 0.250056  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.249812  [    0/ 8693]\n",
      "loss: 0.250127  [ 1024/ 8693]\n",
      "loss: 0.249757  [ 2048/ 8693]\n",
      "loss: 0.250080  [ 3072/ 8693]\n",
      "loss: 0.249982  [ 4096/ 8693]\n",
      "loss: 0.249972  [ 5120/ 8693]\n",
      "loss: 0.250057  [ 6144/ 8693]\n",
      "loss: 0.249973  [ 7168/ 8693]\n",
      "loss: 0.249961  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.249843  [    0/ 8693]\n",
      "loss: 0.249883  [ 1024/ 8693]\n",
      "loss: 0.250285  [ 2048/ 8693]\n",
      "loss: 0.250333  [ 3072/ 8693]\n",
      "loss: 0.250077  [ 4096/ 8693]\n",
      "loss: 0.249908  [ 5120/ 8693]\n",
      "loss: 0.250774  [ 6144/ 8693]\n",
      "loss: 0.250014  [ 7168/ 8693]\n",
      "loss: 0.249744  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.250238  [    0/ 8693]\n",
      "loss: 0.249834  [ 1024/ 8693]\n",
      "loss: 0.250229  [ 2048/ 8693]\n",
      "loss: 0.249797  [ 3072/ 8693]\n",
      "loss: 0.249899  [ 4096/ 8693]\n",
      "loss: 0.250018  [ 5120/ 8693]\n",
      "loss: 0.250023  [ 6144/ 8693]\n",
      "loss: 0.250536  [ 7168/ 8693]\n",
      "loss: 0.249622  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001956 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.250026  [    0/ 8693]\n",
      "loss: 0.249655  [ 1024/ 8693]\n",
      "loss: 0.249741  [ 2048/ 8693]\n",
      "loss: 0.249824  [ 3072/ 8693]\n",
      "loss: 0.249716  [ 4096/ 8693]\n",
      "loss: 0.249261  [ 5120/ 8693]\n",
      "loss: 0.250284  [ 6144/ 8693]\n",
      "loss: 0.250025  [ 7168/ 8693]\n",
      "loss: 0.250245  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.249882  [    0/ 8693]\n",
      "loss: 0.250483  [ 1024/ 8693]\n",
      "loss: 0.250203  [ 2048/ 8693]\n",
      "loss: 0.250006  [ 3072/ 8693]\n",
      "loss: 0.250041  [ 4096/ 8693]\n",
      "loss: 0.249353  [ 5120/ 8693]\n",
      "loss: 0.250012  [ 6144/ 8693]\n",
      "loss: 0.249745  [ 7168/ 8693]\n",
      "loss: 0.250324  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.249458  [    0/ 8693]\n",
      "loss: 0.249447  [ 1024/ 8693]\n",
      "loss: 0.249887  [ 2048/ 8693]\n",
      "loss: 0.250021  [ 3072/ 8693]\n",
      "loss: 0.249652  [ 4096/ 8693]\n",
      "loss: 0.250530  [ 5120/ 8693]\n",
      "loss: 0.249554  [ 6144/ 8693]\n",
      "loss: 0.250581  [ 7168/ 8693]\n",
      "loss: 0.250079  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.250070  [    0/ 8693]\n",
      "loss: 0.250221  [ 1024/ 8693]\n",
      "loss: 0.250175  [ 2048/ 8693]\n",
      "loss: 0.249432  [ 3072/ 8693]\n",
      "loss: 0.250011  [ 4096/ 8693]\n",
      "loss: 0.250196  [ 5120/ 8693]\n",
      "loss: 0.249555  [ 6144/ 8693]\n",
      "loss: 0.249759  [ 7168/ 8693]\n",
      "loss: 0.250074  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.250520  [    0/ 8693]\n",
      "loss: 0.250206  [ 1024/ 8693]\n",
      "loss: 0.250085  [ 2048/ 8693]\n",
      "loss: 0.250013  [ 3072/ 8693]\n",
      "loss: 0.250008  [ 4096/ 8693]\n",
      "loss: 0.250000  [ 5120/ 8693]\n",
      "loss: 0.249988  [ 6144/ 8693]\n",
      "loss: 0.249880  [ 7168/ 8693]\n",
      "loss: 0.249770  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.250319  [    0/ 8693]\n",
      "loss: 0.250129  [ 1024/ 8693]\n",
      "loss: 0.249407  [ 2048/ 8693]\n",
      "loss: 0.249718  [ 3072/ 8693]\n",
      "loss: 0.250029  [ 4096/ 8693]\n",
      "loss: 0.249844  [ 5120/ 8693]\n",
      "loss: 0.249941  [ 6144/ 8693]\n",
      "loss: 0.249653  [ 7168/ 8693]\n",
      "loss: 0.250302  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001956 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.250186  [    0/ 8693]\n",
      "loss: 0.250316  [ 1024/ 8693]\n",
      "loss: 0.249790  [ 2048/ 8693]\n",
      "loss: 0.249832  [ 3072/ 8693]\n",
      "loss: 0.249504  [ 4096/ 8693]\n",
      "loss: 0.250014  [ 5120/ 8693]\n",
      "loss: 0.250013  [ 6144/ 8693]\n",
      "loss: 0.249711  [ 7168/ 8693]\n",
      "loss: 0.250313  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.249681  [    0/ 8693]\n",
      "loss: 0.249821  [ 1024/ 8693]\n",
      "loss: 0.250412  [ 2048/ 8693]\n",
      "loss: 0.250100  [ 3072/ 8693]\n",
      "loss: 0.250566  [ 4096/ 8693]\n",
      "loss: 0.249460  [ 5120/ 8693]\n",
      "loss: 0.250030  [ 6144/ 8693]\n",
      "loss: 0.250497  [ 7168/ 8693]\n",
      "loss: 0.249946  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.250293  [    0/ 8693]\n",
      "loss: 0.249923  [ 1024/ 8693]\n",
      "loss: 0.250050  [ 2048/ 8693]\n",
      "loss: 0.250058  [ 3072/ 8693]\n",
      "loss: 0.249951  [ 4096/ 8693]\n",
      "loss: 0.250005  [ 5120/ 8693]\n",
      "loss: 0.250072  [ 6144/ 8693]\n",
      "loss: 0.250253  [ 7168/ 8693]\n",
      "loss: 0.250022  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.249884  [    0/ 8693]\n",
      "loss: 0.250732  [ 1024/ 8693]\n",
      "loss: 0.250049  [ 2048/ 8693]\n",
      "loss: 0.250239  [ 3072/ 8693]\n",
      "loss: 0.249969  [ 4096/ 8693]\n",
      "loss: 0.250189  [ 5120/ 8693]\n",
      "loss: 0.249771  [ 6144/ 8693]\n",
      "loss: 0.249847  [ 7168/ 8693]\n",
      "loss: 0.250007  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.249961  [    0/ 8693]\n",
      "loss: 0.250014  [ 1024/ 8693]\n",
      "loss: 0.250093  [ 2048/ 8693]\n",
      "loss: 0.250256  [ 3072/ 8693]\n",
      "loss: 0.249872  [ 4096/ 8693]\n",
      "loss: 0.249633  [ 5120/ 8693]\n",
      "loss: 0.249866  [ 6144/ 8693]\n",
      "loss: 0.249946  [ 7168/ 8693]\n",
      "loss: 0.250158  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.250276  [    0/ 8693]\n",
      "loss: 0.249840  [ 1024/ 8693]\n",
      "loss: 0.250280  [ 2048/ 8693]\n",
      "loss: 0.249843  [ 3072/ 8693]\n",
      "loss: 0.250202  [ 4096/ 8693]\n",
      "loss: 0.249687  [ 5120/ 8693]\n",
      "loss: 0.250118  [ 6144/ 8693]\n",
      "loss: 0.250096  [ 7168/ 8693]\n",
      "loss: 0.250009  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.250009  [    0/ 8693]\n",
      "loss: 0.250059  [ 1024/ 8693]\n",
      "loss: 0.249959  [ 2048/ 8693]\n",
      "loss: 0.249844  [ 3072/ 8693]\n",
      "loss: 0.249889  [ 4096/ 8693]\n",
      "loss: 0.249700  [ 5120/ 8693]\n",
      "loss: 0.250304  [ 6144/ 8693]\n",
      "loss: 0.249812  [ 7168/ 8693]\n",
      "loss: 0.249531  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.249248  [    0/ 8693]\n",
      "loss: 0.249499  [ 1024/ 8693]\n",
      "loss: 0.250469  [ 2048/ 8693]\n",
      "loss: 0.250171  [ 3072/ 8693]\n",
      "loss: 0.250416  [ 4096/ 8693]\n",
      "loss: 0.250240  [ 5120/ 8693]\n",
      "loss: 0.249678  [ 6144/ 8693]\n",
      "loss: 0.249962  [ 7168/ 8693]\n",
      "loss: 0.249874  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.249640  [    0/ 8693]\n",
      "loss: 0.249956  [ 1024/ 8693]\n",
      "loss: 0.250707  [ 2048/ 8693]\n",
      "loss: 0.250065  [ 3072/ 8693]\n",
      "loss: 0.250304  [ 4096/ 8693]\n",
      "loss: 0.250127  [ 5120/ 8693]\n",
      "loss: 0.250299  [ 6144/ 8693]\n",
      "loss: 0.250050  [ 7168/ 8693]\n",
      "loss: 0.249970  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001956 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.250196  [    0/ 8693]\n",
      "loss: 0.249963  [ 1024/ 8693]\n",
      "loss: 0.250172  [ 2048/ 8693]\n",
      "loss: 0.250297  [ 3072/ 8693]\n",
      "loss: 0.249696  [ 4096/ 8693]\n",
      "loss: 0.250005  [ 5120/ 8693]\n",
      "loss: 0.249744  [ 6144/ 8693]\n",
      "loss: 0.249471  [ 7168/ 8693]\n",
      "loss: 0.249683  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.250408  [    0/ 8693]\n",
      "loss: 0.250017  [ 1024/ 8693]\n",
      "loss: 0.250021  [ 2048/ 8693]\n",
      "loss: 0.250441  [ 3072/ 8693]\n",
      "loss: 0.250564  [ 4096/ 8693]\n",
      "loss: 0.250131  [ 5120/ 8693]\n",
      "loss: 0.250565  [ 6144/ 8693]\n",
      "loss: 0.250543  [ 7168/ 8693]\n",
      "loss: 0.249699  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.250504  [    0/ 8693]\n",
      "loss: 0.250069  [ 1024/ 8693]\n",
      "loss: 0.249523  [ 2048/ 8693]\n",
      "loss: 0.250010  [ 3072/ 8693]\n",
      "loss: 0.250425  [ 4096/ 8693]\n",
      "loss: 0.249521  [ 5120/ 8693]\n",
      "loss: 0.249904  [ 6144/ 8693]\n",
      "loss: 0.250009  [ 7168/ 8693]\n",
      "loss: 0.250050  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.249822  [    0/ 8693]\n",
      "loss: 0.249572  [ 1024/ 8693]\n",
      "loss: 0.250383  [ 2048/ 8693]\n",
      "loss: 0.250497  [ 3072/ 8693]\n",
      "loss: 0.250043  [ 4096/ 8693]\n",
      "loss: 0.250075  [ 5120/ 8693]\n",
      "loss: 0.249922  [ 6144/ 8693]\n",
      "loss: 0.249583  [ 7168/ 8693]\n",
      "loss: 0.250158  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.249377  [    0/ 8693]\n",
      "loss: 0.249803  [ 1024/ 8693]\n",
      "loss: 0.249710  [ 2048/ 8693]\n",
      "loss: 0.250031  [ 3072/ 8693]\n",
      "loss: 0.250123  [ 4096/ 8693]\n",
      "loss: 0.250718  [ 5120/ 8693]\n",
      "loss: 0.249543  [ 6144/ 8693]\n",
      "loss: 0.249861  [ 7168/ 8693]\n",
      "loss: 0.250268  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.249949  [    0/ 8693]\n",
      "loss: 0.250295  [ 1024/ 8693]\n",
      "loss: 0.249515  [ 2048/ 8693]\n",
      "loss: 0.249679  [ 3072/ 8693]\n",
      "loss: 0.249685  [ 4096/ 8693]\n",
      "loss: 0.250361  [ 5120/ 8693]\n",
      "loss: 0.250310  [ 6144/ 8693]\n",
      "loss: 0.250052  [ 7168/ 8693]\n",
      "loss: 0.250163  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.250115  [    0/ 8693]\n",
      "loss: 0.250497  [ 1024/ 8693]\n",
      "loss: 0.250052  [ 2048/ 8693]\n",
      "loss: 0.250043  [ 3072/ 8693]\n",
      "loss: 0.249808  [ 4096/ 8693]\n",
      "loss: 0.250242  [ 5120/ 8693]\n",
      "loss: 0.249663  [ 6144/ 8693]\n",
      "loss: 0.250105  [ 7168/ 8693]\n",
      "loss: 0.249377  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.250411  [    0/ 8693]\n",
      "loss: 0.249165  [ 1024/ 8693]\n",
      "loss: 0.249745  [ 2048/ 8693]\n",
      "loss: 0.249909  [ 3072/ 8693]\n",
      "loss: 0.249408  [ 4096/ 8693]\n",
      "loss: 0.249970  [ 5120/ 8693]\n",
      "loss: 0.250047  [ 6144/ 8693]\n",
      "loss: 0.249746  [ 7168/ 8693]\n",
      "loss: 0.250099  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001956 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.249811  [    0/ 8693]\n",
      "loss: 0.249842  [ 1024/ 8693]\n",
      "loss: 0.250456  [ 2048/ 8693]\n",
      "loss: 0.249576  [ 3072/ 8693]\n",
      "loss: 0.250022  [ 4096/ 8693]\n",
      "loss: 0.250716  [ 5120/ 8693]\n",
      "loss: 0.250794  [ 6144/ 8693]\n",
      "loss: 0.250196  [ 7168/ 8693]\n",
      "loss: 0.250912  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.249877  [    0/ 8693]\n",
      "loss: 0.249705  [ 1024/ 8693]\n",
      "loss: 0.249874  [ 2048/ 8693]\n",
      "loss: 0.250169  [ 3072/ 8693]\n",
      "loss: 0.249867  [ 4096/ 8693]\n",
      "loss: 0.249857  [ 5120/ 8693]\n",
      "loss: 0.250633  [ 6144/ 8693]\n",
      "loss: 0.249879  [ 7168/ 8693]\n",
      "loss: 0.250415  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.250257  [    0/ 8693]\n",
      "loss: 0.250136  [ 1024/ 8693]\n",
      "loss: 0.250205  [ 2048/ 8693]\n",
      "loss: 0.249801  [ 3072/ 8693]\n",
      "loss: 0.249967  [ 4096/ 8693]\n",
      "loss: 0.249841  [ 5120/ 8693]\n",
      "loss: 0.250146  [ 6144/ 8693]\n",
      "loss: 0.250007  [ 7168/ 8693]\n",
      "loss: 0.250480  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.249248  [    0/ 8693]\n",
      "loss: 0.249896  [ 1024/ 8693]\n",
      "loss: 0.249628  [ 2048/ 8693]\n",
      "loss: 0.249835  [ 3072/ 8693]\n",
      "loss: 0.250062  [ 4096/ 8693]\n",
      "loss: 0.249911  [ 5120/ 8693]\n",
      "loss: 0.250062  [ 6144/ 8693]\n",
      "loss: 0.249960  [ 7168/ 8693]\n",
      "loss: 0.250219  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.250617  [    0/ 8693]\n",
      "loss: 0.250519  [ 1024/ 8693]\n",
      "loss: 0.250216  [ 2048/ 8693]\n",
      "loss: 0.249888  [ 3072/ 8693]\n",
      "loss: 0.250350  [ 4096/ 8693]\n",
      "loss: 0.249500  [ 5120/ 8693]\n",
      "loss: 0.249626  [ 6144/ 8693]\n",
      "loss: 0.249783  [ 7168/ 8693]\n",
      "loss: 0.250377  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.250075  [    0/ 8693]\n",
      "loss: 0.249697  [ 1024/ 8693]\n",
      "loss: 0.250217  [ 2048/ 8693]\n",
      "loss: 0.250143  [ 3072/ 8693]\n",
      "loss: 0.250214  [ 4096/ 8693]\n",
      "loss: 0.250338  [ 5120/ 8693]\n",
      "loss: 0.249448  [ 6144/ 8693]\n",
      "loss: 0.249944  [ 7168/ 8693]\n",
      "loss: 0.249875  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.250018  [    0/ 8693]\n",
      "loss: 0.250218  [ 1024/ 8693]\n",
      "loss: 0.250230  [ 2048/ 8693]\n",
      "loss: 0.249826  [ 3072/ 8693]\n",
      "loss: 0.249450  [ 4096/ 8693]\n",
      "loss: 0.249334  [ 5120/ 8693]\n",
      "loss: 0.249855  [ 6144/ 8693]\n",
      "loss: 0.249915  [ 7168/ 8693]\n",
      "loss: 0.249888  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.249796  [    0/ 8693]\n",
      "loss: 0.250058  [ 1024/ 8693]\n",
      "loss: 0.250326  [ 2048/ 8693]\n",
      "loss: 0.249808  [ 3072/ 8693]\n",
      "loss: 0.250012  [ 4096/ 8693]\n",
      "loss: 0.249844  [ 5120/ 8693]\n",
      "loss: 0.250184  [ 6144/ 8693]\n",
      "loss: 0.250076  [ 7168/ 8693]\n",
      "loss: 0.249591  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.249832  [    0/ 8693]\n",
      "loss: 0.250755  [ 1024/ 8693]\n",
      "loss: 0.250305  [ 2048/ 8693]\n",
      "loss: 0.250237  [ 3072/ 8693]\n",
      "loss: 0.250449  [ 4096/ 8693]\n",
      "loss: 0.249887  [ 5120/ 8693]\n",
      "loss: 0.250251  [ 6144/ 8693]\n",
      "loss: 0.250006  [ 7168/ 8693]\n",
      "loss: 0.250369  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.249570  [    0/ 8693]\n",
      "loss: 0.250443  [ 1024/ 8693]\n",
      "loss: 0.250022  [ 2048/ 8693]\n",
      "loss: 0.249416  [ 3072/ 8693]\n",
      "loss: 0.249610  [ 4096/ 8693]\n",
      "loss: 0.249950  [ 5120/ 8693]\n",
      "loss: 0.249807  [ 6144/ 8693]\n",
      "loss: 0.249889  [ 7168/ 8693]\n",
      "loss: 0.250313  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.250127  [    0/ 8693]\n",
      "loss: 0.250146  [ 1024/ 8693]\n",
      "loss: 0.250119  [ 2048/ 8693]\n",
      "loss: 0.249805  [ 3072/ 8693]\n",
      "loss: 0.249687  [ 4096/ 8693]\n",
      "loss: 0.250490  [ 5120/ 8693]\n",
      "loss: 0.250481  [ 6144/ 8693]\n",
      "loss: 0.249138  [ 7168/ 8693]\n",
      "loss: 0.250223  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.249885  [    0/ 8693]\n",
      "loss: 0.249371  [ 1024/ 8693]\n",
      "loss: 0.249069  [ 2048/ 8693]\n",
      "loss: 0.250663  [ 3072/ 8693]\n",
      "loss: 0.249569  [ 4096/ 8693]\n",
      "loss: 0.249203  [ 5120/ 8693]\n",
      "loss: 0.250128  [ 6144/ 8693]\n",
      "loss: 0.250375  [ 7168/ 8693]\n",
      "loss: 0.250022  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.249489  [    0/ 8693]\n",
      "loss: 0.250731  [ 1024/ 8693]\n",
      "loss: 0.250192  [ 2048/ 8693]\n",
      "loss: 0.249712  [ 3072/ 8693]\n",
      "loss: 0.249752  [ 4096/ 8693]\n",
      "loss: 0.250283  [ 5120/ 8693]\n",
      "loss: 0.250275  [ 6144/ 8693]\n",
      "loss: 0.250359  [ 7168/ 8693]\n",
      "loss: 0.249918  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.250442  [    0/ 8693]\n",
      "loss: 0.249833  [ 1024/ 8693]\n",
      "loss: 0.249861  [ 2048/ 8693]\n",
      "loss: 0.250152  [ 3072/ 8693]\n",
      "loss: 0.249830  [ 4096/ 8693]\n",
      "loss: 0.250324  [ 5120/ 8693]\n",
      "loss: 0.250446  [ 6144/ 8693]\n",
      "loss: 0.250131  [ 7168/ 8693]\n",
      "loss: 0.250071  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.249626  [    0/ 8693]\n",
      "loss: 0.249665  [ 1024/ 8693]\n",
      "loss: 0.250066  [ 2048/ 8693]\n",
      "loss: 0.250499  [ 3072/ 8693]\n",
      "loss: 0.249645  [ 4096/ 8693]\n",
      "loss: 0.250013  [ 5120/ 8693]\n",
      "loss: 0.249959  [ 6144/ 8693]\n",
      "loss: 0.250008  [ 7168/ 8693]\n",
      "loss: 0.250010  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.249959  [    0/ 8693]\n",
      "loss: 0.250123  [ 1024/ 8693]\n",
      "loss: 0.250075  [ 2048/ 8693]\n",
      "loss: 0.250247  [ 3072/ 8693]\n",
      "loss: 0.249865  [ 4096/ 8693]\n",
      "loss: 0.250521  [ 5120/ 8693]\n",
      "loss: 0.250020  [ 6144/ 8693]\n",
      "loss: 0.251174  [ 7168/ 8693]\n",
      "loss: 0.249443  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.249636  [    0/ 8693]\n",
      "loss: 0.249558  [ 1024/ 8693]\n",
      "loss: 0.249924  [ 2048/ 8693]\n",
      "loss: 0.249856  [ 3072/ 8693]\n",
      "loss: 0.250259  [ 4096/ 8693]\n",
      "loss: 0.250010  [ 5120/ 8693]\n",
      "loss: 0.250114  [ 6144/ 8693]\n",
      "loss: 0.250062  [ 7168/ 8693]\n",
      "loss: 0.250565  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.250009  [    0/ 8693]\n",
      "loss: 0.249957  [ 1024/ 8693]\n",
      "loss: 0.250119  [ 2048/ 8693]\n",
      "loss: 0.249704  [ 3072/ 8693]\n",
      "loss: 0.249789  [ 4096/ 8693]\n",
      "loss: 0.250234  [ 5120/ 8693]\n",
      "loss: 0.249959  [ 6144/ 8693]\n",
      "loss: 0.250302  [ 7168/ 8693]\n",
      "loss: 0.249819  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.249862  [    0/ 8693]\n",
      "loss: 0.249899  [ 1024/ 8693]\n",
      "loss: 0.249876  [ 2048/ 8693]\n",
      "loss: 0.249612  [ 3072/ 8693]\n",
      "loss: 0.249865  [ 4096/ 8693]\n",
      "loss: 0.249667  [ 5120/ 8693]\n",
      "loss: 0.249735  [ 6144/ 8693]\n",
      "loss: 0.250172  [ 7168/ 8693]\n",
      "loss: 0.250316  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.249570  [    0/ 8693]\n",
      "loss: 0.249587  [ 1024/ 8693]\n",
      "loss: 0.249857  [ 2048/ 8693]\n",
      "loss: 0.249963  [ 3072/ 8693]\n",
      "loss: 0.250213  [ 4096/ 8693]\n",
      "loss: 0.249821  [ 5120/ 8693]\n",
      "loss: 0.250578  [ 6144/ 8693]\n",
      "loss: 0.250247  [ 7168/ 8693]\n",
      "loss: 0.249959  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.250160  [    0/ 8693]\n",
      "loss: 0.250528  [ 1024/ 8693]\n",
      "loss: 0.250235  [ 2048/ 8693]\n",
      "loss: 0.249857  [ 3072/ 8693]\n",
      "loss: 0.249631  [ 4096/ 8693]\n",
      "loss: 0.250138  [ 5120/ 8693]\n",
      "loss: 0.250290  [ 6144/ 8693]\n",
      "loss: 0.249897  [ 7168/ 8693]\n",
      "loss: 0.250054  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.249615  [    0/ 8693]\n",
      "loss: 0.249079  [ 1024/ 8693]\n",
      "loss: 0.250072  [ 2048/ 8693]\n",
      "loss: 0.250172  [ 3072/ 8693]\n",
      "loss: 0.249594  [ 4096/ 8693]\n",
      "loss: 0.250015  [ 5120/ 8693]\n",
      "loss: 0.249804  [ 6144/ 8693]\n",
      "loss: 0.250284  [ 7168/ 8693]\n",
      "loss: 0.250350  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.249436  [    0/ 8693]\n",
      "loss: 0.250929  [ 1024/ 8693]\n",
      "loss: 0.250102  [ 2048/ 8693]\n",
      "loss: 0.250171  [ 3072/ 8693]\n",
      "loss: 0.249838  [ 4096/ 8693]\n",
      "loss: 0.249919  [ 5120/ 8693]\n",
      "loss: 0.250010  [ 6144/ 8693]\n",
      "loss: 0.250164  [ 7168/ 8693]\n",
      "loss: 0.250405  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.249735  [    0/ 8693]\n",
      "loss: 0.249156  [ 1024/ 8693]\n",
      "loss: 0.249638  [ 2048/ 8693]\n",
      "loss: 0.250081  [ 3072/ 8693]\n",
      "loss: 0.250149  [ 4096/ 8693]\n",
      "loss: 0.250332  [ 5120/ 8693]\n",
      "loss: 0.249719  [ 6144/ 8693]\n",
      "loss: 0.250130  [ 7168/ 8693]\n",
      "loss: 0.249702  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.249905  [    0/ 8693]\n",
      "loss: 0.250486  [ 1024/ 8693]\n",
      "loss: 0.249761  [ 2048/ 8693]\n",
      "loss: 0.250272  [ 3072/ 8693]\n",
      "loss: 0.249898  [ 4096/ 8693]\n",
      "loss: 0.250203  [ 5120/ 8693]\n",
      "loss: 0.250196  [ 6144/ 8693]\n",
      "loss: 0.250240  [ 7168/ 8693]\n",
      "loss: 0.249300  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.250106  [    0/ 8693]\n",
      "loss: 0.250310  [ 1024/ 8693]\n",
      "loss: 0.250077  [ 2048/ 8693]\n",
      "loss: 0.250127  [ 3072/ 8693]\n",
      "loss: 0.250310  [ 4096/ 8693]\n",
      "loss: 0.249666  [ 5120/ 8693]\n",
      "loss: 0.250008  [ 6144/ 8693]\n",
      "loss: 0.249963  [ 7168/ 8693]\n",
      "loss: 0.249908  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.249635  [    0/ 8693]\n",
      "loss: 0.250426  [ 1024/ 8693]\n",
      "loss: 0.250144  [ 2048/ 8693]\n",
      "loss: 0.249302  [ 3072/ 8693]\n",
      "loss: 0.249948  [ 4096/ 8693]\n",
      "loss: 0.250086  [ 5120/ 8693]\n",
      "loss: 0.250067  [ 6144/ 8693]\n",
      "loss: 0.249959  [ 7168/ 8693]\n",
      "loss: 0.249907  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.250431  [    0/ 8693]\n",
      "loss: 0.249862  [ 1024/ 8693]\n",
      "loss: 0.249572  [ 2048/ 8693]\n",
      "loss: 0.250083  [ 3072/ 8693]\n",
      "loss: 0.250396  [ 4096/ 8693]\n",
      "loss: 0.250194  [ 5120/ 8693]\n",
      "loss: 0.250472  [ 6144/ 8693]\n",
      "loss: 0.249742  [ 7168/ 8693]\n",
      "loss: 0.249912  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.250304  [    0/ 8693]\n",
      "loss: 0.249961  [ 1024/ 8693]\n",
      "loss: 0.249666  [ 2048/ 8693]\n",
      "loss: 0.250299  [ 3072/ 8693]\n",
      "loss: 0.250544  [ 4096/ 8693]\n",
      "loss: 0.250295  [ 5120/ 8693]\n",
      "loss: 0.249530  [ 6144/ 8693]\n",
      "loss: 0.250211  [ 7168/ 8693]\n",
      "loss: 0.249892  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.250252  [    0/ 8693]\n",
      "loss: 0.250210  [ 1024/ 8693]\n",
      "loss: 0.249916  [ 2048/ 8693]\n",
      "loss: 0.249727  [ 3072/ 8693]\n",
      "loss: 0.249612  [ 4096/ 8693]\n",
      "loss: 0.250328  [ 5120/ 8693]\n",
      "loss: 0.250571  [ 6144/ 8693]\n",
      "loss: 0.250572  [ 7168/ 8693]\n",
      "loss: 0.250163  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.249906  [    0/ 8693]\n",
      "loss: 0.249644  [ 1024/ 8693]\n",
      "loss: 0.249844  [ 2048/ 8693]\n",
      "loss: 0.250256  [ 3072/ 8693]\n",
      "loss: 0.250326  [ 4096/ 8693]\n",
      "loss: 0.249693  [ 5120/ 8693]\n",
      "loss: 0.250341  [ 6144/ 8693]\n",
      "loss: 0.250104  [ 7168/ 8693]\n",
      "loss: 0.249826  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.249956  [    0/ 8693]\n",
      "loss: 0.249799  [ 1024/ 8693]\n",
      "loss: 0.250268  [ 2048/ 8693]\n",
      "loss: 0.249960  [ 3072/ 8693]\n",
      "loss: 0.250222  [ 4096/ 8693]\n",
      "loss: 0.249910  [ 5120/ 8693]\n",
      "loss: 0.250058  [ 6144/ 8693]\n",
      "loss: 0.249524  [ 7168/ 8693]\n",
      "loss: 0.249831  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.250710  [    0/ 8693]\n",
      "loss: 0.250155  [ 1024/ 8693]\n",
      "loss: 0.249589  [ 2048/ 8693]\n",
      "loss: 0.250563  [ 3072/ 8693]\n",
      "loss: 0.250013  [ 4096/ 8693]\n",
      "loss: 0.250063  [ 5120/ 8693]\n",
      "loss: 0.249874  [ 6144/ 8693]\n",
      "loss: 0.250217  [ 7168/ 8693]\n",
      "loss: 0.249839  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.249654  [    0/ 8693]\n",
      "loss: 0.250143  [ 1024/ 8693]\n",
      "loss: 0.250515  [ 2048/ 8693]\n",
      "loss: 0.250144  [ 3072/ 8693]\n",
      "loss: 0.250092  [ 4096/ 8693]\n",
      "loss: 0.250178  [ 5120/ 8693]\n",
      "loss: 0.250025  [ 6144/ 8693]\n",
      "loss: 0.249945  [ 7168/ 8693]\n",
      "loss: 0.250366  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.250573  [    0/ 8693]\n",
      "loss: 0.249961  [ 1024/ 8693]\n",
      "loss: 0.250179  [ 2048/ 8693]\n",
      "loss: 0.249704  [ 3072/ 8693]\n",
      "loss: 0.250307  [ 4096/ 8693]\n",
      "loss: 0.250202  [ 5120/ 8693]\n",
      "loss: 0.249922  [ 6144/ 8693]\n",
      "loss: 0.250361  [ 7168/ 8693]\n",
      "loss: 0.249864  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.249864  [    0/ 8693]\n",
      "loss: 0.250169  [ 1024/ 8693]\n",
      "loss: 0.249872  [ 2048/ 8693]\n",
      "loss: 0.250004  [ 3072/ 8693]\n",
      "loss: 0.250102  [ 4096/ 8693]\n",
      "loss: 0.249972  [ 5120/ 8693]\n",
      "loss: 0.249983  [ 6144/ 8693]\n",
      "loss: 0.249916  [ 7168/ 8693]\n",
      "loss: 0.249974  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.249924  [    0/ 8693]\n",
      "loss: 0.249341  [ 1024/ 8693]\n",
      "loss: 0.249898  [ 2048/ 8693]\n",
      "loss: 0.250378  [ 3072/ 8693]\n",
      "loss: 0.250395  [ 4096/ 8693]\n",
      "loss: 0.249346  [ 5120/ 8693]\n",
      "loss: 0.249684  [ 6144/ 8693]\n",
      "loss: 0.249837  [ 7168/ 8693]\n",
      "loss: 0.250572  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.250077  [    0/ 8693]\n",
      "loss: 0.250407  [ 1024/ 8693]\n",
      "loss: 0.249708  [ 2048/ 8693]\n",
      "loss: 0.250973  [ 3072/ 8693]\n",
      "loss: 0.250229  [ 4096/ 8693]\n",
      "loss: 0.249968  [ 5120/ 8693]\n",
      "loss: 0.249760  [ 6144/ 8693]\n",
      "loss: 0.250135  [ 7168/ 8693]\n",
      "loss: 0.250083  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.250085  [    0/ 8693]\n",
      "loss: 0.249776  [ 1024/ 8693]\n",
      "loss: 0.250176  [ 2048/ 8693]\n",
      "loss: 0.249684  [ 3072/ 8693]\n",
      "loss: 0.249898  [ 4096/ 8693]\n",
      "loss: 0.249897  [ 5120/ 8693]\n",
      "loss: 0.250012  [ 6144/ 8693]\n",
      "loss: 0.249956  [ 7168/ 8693]\n",
      "loss: 0.250413  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.250018  [    0/ 8693]\n",
      "loss: 0.250100  [ 1024/ 8693]\n",
      "loss: 0.250687  [ 2048/ 8693]\n",
      "loss: 0.250489  [ 3072/ 8693]\n",
      "loss: 0.249498  [ 4096/ 8693]\n",
      "loss: 0.248891  [ 5120/ 8693]\n",
      "loss: 0.250688  [ 6144/ 8693]\n",
      "loss: 0.251376  [ 7168/ 8693]\n",
      "loss: 0.249697  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.249654  [    0/ 8693]\n",
      "loss: 0.250233  [ 1024/ 8693]\n",
      "loss: 0.249328  [ 2048/ 8693]\n",
      "loss: 0.249501  [ 3072/ 8693]\n",
      "loss: 0.249784  [ 4096/ 8693]\n",
      "loss: 0.250252  [ 5120/ 8693]\n",
      "loss: 0.250149  [ 6144/ 8693]\n",
      "loss: 0.249908  [ 7168/ 8693]\n",
      "loss: 0.249798  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.250094  [    0/ 8693]\n",
      "loss: 0.250043  [ 1024/ 8693]\n",
      "loss: 0.249834  [ 2048/ 8693]\n",
      "loss: 0.250208  [ 3072/ 8693]\n",
      "loss: 0.249894  [ 4096/ 8693]\n",
      "loss: 0.249901  [ 5120/ 8693]\n",
      "loss: 0.249896  [ 6144/ 8693]\n",
      "loss: 0.249827  [ 7168/ 8693]\n",
      "loss: 0.249523  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.250219  [    0/ 8693]\n",
      "loss: 0.250169  [ 1024/ 8693]\n",
      "loss: 0.249955  [ 2048/ 8693]\n",
      "loss: 0.250275  [ 3072/ 8693]\n",
      "loss: 0.249728  [ 4096/ 8693]\n",
      "loss: 0.249654  [ 5120/ 8693]\n",
      "loss: 0.250538  [ 6144/ 8693]\n",
      "loss: 0.250130  [ 7168/ 8693]\n",
      "loss: 0.250474  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.249459  [    0/ 8693]\n",
      "loss: 0.250013  [ 1024/ 8693]\n",
      "loss: 0.249583  [ 2048/ 8693]\n",
      "loss: 0.250307  [ 3072/ 8693]\n",
      "loss: 0.250329  [ 4096/ 8693]\n",
      "loss: 0.250281  [ 5120/ 8693]\n",
      "loss: 0.249472  [ 6144/ 8693]\n",
      "loss: 0.250255  [ 7168/ 8693]\n",
      "loss: 0.249899  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.249903  [    0/ 8693]\n",
      "loss: 0.250401  [ 1024/ 8693]\n",
      "loss: 0.249968  [ 2048/ 8693]\n",
      "loss: 0.249932  [ 3072/ 8693]\n",
      "loss: 0.249959  [ 4096/ 8693]\n",
      "loss: 0.250648  [ 5120/ 8693]\n",
      "loss: 0.249740  [ 6144/ 8693]\n",
      "loss: 0.250234  [ 7168/ 8693]\n",
      "loss: 0.250156  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.250140  [    0/ 8693]\n",
      "loss: 0.249953  [ 1024/ 8693]\n",
      "loss: 0.250811  [ 2048/ 8693]\n",
      "loss: 0.249335  [ 3072/ 8693]\n",
      "loss: 0.249485  [ 4096/ 8693]\n",
      "loss: 0.250140  [ 5120/ 8693]\n",
      "loss: 0.250251  [ 6144/ 8693]\n",
      "loss: 0.250157  [ 7168/ 8693]\n",
      "loss: 0.249775  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.250009  [    0/ 8693]\n",
      "loss: 0.249681  [ 1024/ 8693]\n",
      "loss: 0.249940  [ 2048/ 8693]\n",
      "loss: 0.250033  [ 3072/ 8693]\n",
      "loss: 0.250459  [ 4096/ 8693]\n",
      "loss: 0.249597  [ 5120/ 8693]\n",
      "loss: 0.249931  [ 6144/ 8693]\n",
      "loss: 0.249922  [ 7168/ 8693]\n",
      "loss: 0.249963  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.249914  [    0/ 8693]\n",
      "loss: 0.250346  [ 1024/ 8693]\n",
      "loss: 0.249959  [ 2048/ 8693]\n",
      "loss: 0.249811  [ 3072/ 8693]\n",
      "loss: 0.249306  [ 4096/ 8693]\n",
      "loss: 0.250084  [ 5120/ 8693]\n",
      "loss: 0.250013  [ 6144/ 8693]\n",
      "loss: 0.250257  [ 7168/ 8693]\n",
      "loss: 0.249568  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.250008  [    0/ 8693]\n",
      "loss: 0.250370  [ 1024/ 8693]\n",
      "loss: 0.249415  [ 2048/ 8693]\n",
      "loss: 0.249481  [ 3072/ 8693]\n",
      "loss: 0.250265  [ 4096/ 8693]\n",
      "loss: 0.249454  [ 5120/ 8693]\n",
      "loss: 0.249445  [ 6144/ 8693]\n",
      "loss: 0.249605  [ 7168/ 8693]\n",
      "loss: 0.250225  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.249509  [    0/ 8693]\n",
      "loss: 0.250081  [ 1024/ 8693]\n",
      "loss: 0.250229  [ 2048/ 8693]\n",
      "loss: 0.249802  [ 3072/ 8693]\n",
      "loss: 0.250087  [ 4096/ 8693]\n",
      "loss: 0.250644  [ 5120/ 8693]\n",
      "loss: 0.250056  [ 6144/ 8693]\n",
      "loss: 0.249629  [ 7168/ 8693]\n",
      "loss: 0.250357  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.249588  [    0/ 8693]\n",
      "loss: 0.250648  [ 1024/ 8693]\n",
      "loss: 0.250177  [ 2048/ 8693]\n",
      "loss: 0.249737  [ 3072/ 8693]\n",
      "loss: 0.249501  [ 4096/ 8693]\n",
      "loss: 0.249774  [ 5120/ 8693]\n",
      "loss: 0.249438  [ 6144/ 8693]\n",
      "loss: 0.250341  [ 7168/ 8693]\n",
      "loss: 0.249715  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.249775  [    0/ 8693]\n",
      "loss: 0.250124  [ 1024/ 8693]\n",
      "loss: 0.250192  [ 2048/ 8693]\n",
      "loss: 0.250324  [ 3072/ 8693]\n",
      "loss: 0.249885  [ 4096/ 8693]\n",
      "loss: 0.249269  [ 5120/ 8693]\n",
      "loss: 0.249887  [ 6144/ 8693]\n",
      "loss: 0.249712  [ 7168/ 8693]\n",
      "loss: 0.249957  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.249958  [    0/ 8693]\n",
      "loss: 0.250267  [ 1024/ 8693]\n",
      "loss: 0.249924  [ 2048/ 8693]\n",
      "loss: 0.249975  [ 3072/ 8693]\n",
      "loss: 0.250073  [ 4096/ 8693]\n",
      "loss: 0.249814  [ 5120/ 8693]\n",
      "loss: 0.250058  [ 6144/ 8693]\n",
      "loss: 0.249832  [ 7168/ 8693]\n",
      "loss: 0.249757  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.250105  [    0/ 8693]\n",
      "loss: 0.249693  [ 1024/ 8693]\n",
      "loss: 0.250953  [ 2048/ 8693]\n",
      "loss: 0.250012  [ 3072/ 8693]\n",
      "loss: 0.249747  [ 4096/ 8693]\n",
      "loss: 0.250116  [ 5120/ 8693]\n",
      "loss: 0.250410  [ 6144/ 8693]\n",
      "loss: 0.250066  [ 7168/ 8693]\n",
      "loss: 0.249710  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.249523  [    0/ 8693]\n",
      "loss: 0.250076  [ 1024/ 8693]\n",
      "loss: 0.250016  [ 2048/ 8693]\n",
      "loss: 0.250022  [ 3072/ 8693]\n",
      "loss: 0.249711  [ 4096/ 8693]\n",
      "loss: 0.249559  [ 5120/ 8693]\n",
      "loss: 0.250155  [ 6144/ 8693]\n",
      "loss: 0.249847  [ 7168/ 8693]\n",
      "loss: 0.249789  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.250021  [    0/ 8693]\n",
      "loss: 0.249810  [ 1024/ 8693]\n",
      "loss: 0.250224  [ 2048/ 8693]\n",
      "loss: 0.249684  [ 3072/ 8693]\n",
      "loss: 0.249261  [ 4096/ 8693]\n",
      "loss: 0.250106  [ 5120/ 8693]\n",
      "loss: 0.250249  [ 6144/ 8693]\n",
      "loss: 0.250477  [ 7168/ 8693]\n",
      "loss: 0.249771  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.249722  [    0/ 8693]\n",
      "loss: 0.250306  [ 1024/ 8693]\n",
      "loss: 0.249295  [ 2048/ 8693]\n",
      "loss: 0.249461  [ 3072/ 8693]\n",
      "loss: 0.249878  [ 4096/ 8693]\n",
      "loss: 0.249952  [ 5120/ 8693]\n",
      "loss: 0.250472  [ 6144/ 8693]\n",
      "loss: 0.250515  [ 7168/ 8693]\n",
      "loss: 0.250376  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.250008  [    0/ 8693]\n",
      "loss: 0.249678  [ 1024/ 8693]\n",
      "loss: 0.249533  [ 2048/ 8693]\n",
      "loss: 0.250246  [ 3072/ 8693]\n",
      "loss: 0.250064  [ 4096/ 8693]\n",
      "loss: 0.250012  [ 5120/ 8693]\n",
      "loss: 0.249335  [ 6144/ 8693]\n",
      "loss: 0.250323  [ 7168/ 8693]\n",
      "loss: 0.250076  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.250192  [    0/ 8693]\n",
      "loss: 0.250013  [ 1024/ 8693]\n",
      "loss: 0.249953  [ 2048/ 8693]\n",
      "loss: 0.250014  [ 3072/ 8693]\n",
      "loss: 0.249860  [ 4096/ 8693]\n",
      "loss: 0.249366  [ 5120/ 8693]\n",
      "loss: 0.250054  [ 6144/ 8693]\n",
      "loss: 0.250184  [ 7168/ 8693]\n",
      "loss: 0.249866  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.250173  [    0/ 8693]\n",
      "loss: 0.249869  [ 1024/ 8693]\n",
      "loss: 0.250417  [ 2048/ 8693]\n",
      "loss: 0.250256  [ 3072/ 8693]\n",
      "loss: 0.250374  [ 4096/ 8693]\n",
      "loss: 0.249825  [ 5120/ 8693]\n",
      "loss: 0.249737  [ 6144/ 8693]\n",
      "loss: 0.249599  [ 7168/ 8693]\n",
      "loss: 0.250152  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.249339  [    0/ 8693]\n",
      "loss: 0.250385  [ 1024/ 8693]\n",
      "loss: 0.249949  [ 2048/ 8693]\n",
      "loss: 0.249950  [ 3072/ 8693]\n",
      "loss: 0.249563  [ 4096/ 8693]\n",
      "loss: 0.249910  [ 5120/ 8693]\n",
      "loss: 0.249781  [ 6144/ 8693]\n",
      "loss: 0.250010  [ 7168/ 8693]\n",
      "loss: 0.249916  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.249865  [    0/ 8693]\n",
      "loss: 0.250109  [ 1024/ 8693]\n",
      "loss: 0.249912  [ 2048/ 8693]\n",
      "loss: 0.250092  [ 3072/ 8693]\n",
      "loss: 0.250058  [ 4096/ 8693]\n",
      "loss: 0.250072  [ 5120/ 8693]\n",
      "loss: 0.249945  [ 6144/ 8693]\n",
      "loss: 0.249754  [ 7168/ 8693]\n",
      "loss: 0.249959  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.250411  [    0/ 8693]\n",
      "loss: 0.250068  [ 1024/ 8693]\n",
      "loss: 0.250447  [ 2048/ 8693]\n",
      "loss: 0.249874  [ 3072/ 8693]\n",
      "loss: 0.249893  [ 4096/ 8693]\n",
      "loss: 0.249974  [ 5120/ 8693]\n",
      "loss: 0.250287  [ 6144/ 8693]\n",
      "loss: 0.249783  [ 7168/ 8693]\n",
      "loss: 0.249666  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.249501  [    0/ 8693]\n",
      "loss: 0.249948  [ 1024/ 8693]\n",
      "loss: 0.249949  [ 2048/ 8693]\n",
      "loss: 0.249820  [ 3072/ 8693]\n",
      "loss: 0.249345  [ 4096/ 8693]\n",
      "loss: 0.249879  [ 5120/ 8693]\n",
      "loss: 0.249789  [ 6144/ 8693]\n",
      "loss: 0.250101  [ 7168/ 8693]\n",
      "loss: 0.250668  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.249470  [    0/ 8693]\n",
      "loss: 0.250208  [ 1024/ 8693]\n",
      "loss: 0.250389  [ 2048/ 8693]\n",
      "loss: 0.249862  [ 3072/ 8693]\n",
      "loss: 0.250009  [ 4096/ 8693]\n",
      "loss: 0.249769  [ 5120/ 8693]\n",
      "loss: 0.249494  [ 6144/ 8693]\n",
      "loss: 0.250242  [ 7168/ 8693]\n",
      "loss: 0.250491  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.250014  [    0/ 8693]\n",
      "loss: 0.250426  [ 1024/ 8693]\n",
      "loss: 0.250099  [ 2048/ 8693]\n",
      "loss: 0.250302  [ 3072/ 8693]\n",
      "loss: 0.250183  [ 4096/ 8693]\n",
      "loss: 0.250444  [ 5120/ 8693]\n",
      "loss: 0.250191  [ 6144/ 8693]\n",
      "loss: 0.249799  [ 7168/ 8693]\n",
      "loss: 0.250278  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.249612  [    0/ 8693]\n",
      "loss: 0.249892  [ 1024/ 8693]\n",
      "loss: 0.249860  [ 2048/ 8693]\n",
      "loss: 0.250265  [ 3072/ 8693]\n",
      "loss: 0.250389  [ 4096/ 8693]\n",
      "loss: 0.249629  [ 5120/ 8693]\n",
      "loss: 0.249734  [ 6144/ 8693]\n",
      "loss: 0.250085  [ 7168/ 8693]\n",
      "loss: 0.250296  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.250221  [    0/ 8693]\n",
      "loss: 0.250017  [ 1024/ 8693]\n",
      "loss: 0.249884  [ 2048/ 8693]\n",
      "loss: 0.250291  [ 3072/ 8693]\n",
      "loss: 0.249562  [ 4096/ 8693]\n",
      "loss: 0.249885  [ 5120/ 8693]\n",
      "loss: 0.249771  [ 6144/ 8693]\n",
      "loss: 0.250599  [ 7168/ 8693]\n",
      "loss: 0.249620  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.250193  [    0/ 8693]\n",
      "loss: 0.249459  [ 1024/ 8693]\n",
      "loss: 0.249313  [ 2048/ 8693]\n",
      "loss: 0.249867  [ 3072/ 8693]\n",
      "loss: 0.250049  [ 4096/ 8693]\n",
      "loss: 0.249877  [ 5120/ 8693]\n",
      "loss: 0.249853  [ 6144/ 8693]\n",
      "loss: 0.249727  [ 7168/ 8693]\n",
      "loss: 0.249384  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.249585  [    0/ 8693]\n",
      "loss: 0.249745  [ 1024/ 8693]\n",
      "loss: 0.249701  [ 2048/ 8693]\n",
      "loss: 0.250724  [ 3072/ 8693]\n",
      "loss: 0.250187  [ 4096/ 8693]\n",
      "loss: 0.249804  [ 5120/ 8693]\n",
      "loss: 0.249753  [ 6144/ 8693]\n",
      "loss: 0.249280  [ 7168/ 8693]\n",
      "loss: 0.250433  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.250017  [    0/ 8693]\n",
      "loss: 0.250423  [ 1024/ 8693]\n",
      "loss: 0.250693  [ 2048/ 8693]\n",
      "loss: 0.250455  [ 3072/ 8693]\n",
      "loss: 0.250078  [ 4096/ 8693]\n",
      "loss: 0.249516  [ 5120/ 8693]\n",
      "loss: 0.249827  [ 6144/ 8693]\n",
      "loss: 0.250441  [ 7168/ 8693]\n",
      "loss: 0.250207  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.249283  [    0/ 8693]\n",
      "loss: 0.249310  [ 1024/ 8693]\n",
      "loss: 0.250589  [ 2048/ 8693]\n",
      "loss: 0.250380  [ 3072/ 8693]\n",
      "loss: 0.250100  [ 4096/ 8693]\n",
      "loss: 0.250429  [ 5120/ 8693]\n",
      "loss: 0.249681  [ 6144/ 8693]\n",
      "loss: 0.249660  [ 7168/ 8693]\n",
      "loss: 0.250008  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.250055  [    0/ 8693]\n",
      "loss: 0.250062  [ 1024/ 8693]\n",
      "loss: 0.250344  [ 2048/ 8693]\n",
      "loss: 0.250182  [ 3072/ 8693]\n",
      "loss: 0.250086  [ 4096/ 8693]\n",
      "loss: 0.249438  [ 5120/ 8693]\n",
      "loss: 0.249242  [ 6144/ 8693]\n",
      "loss: 0.249953  [ 7168/ 8693]\n",
      "loss: 0.250141  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.249897  [    0/ 8693]\n",
      "loss: 0.249518  [ 1024/ 8693]\n",
      "loss: 0.249224  [ 2048/ 8693]\n",
      "loss: 0.250486  [ 3072/ 8693]\n",
      "loss: 0.249297  [ 4096/ 8693]\n",
      "loss: 0.249120  [ 5120/ 8693]\n",
      "loss: 0.250488  [ 6144/ 8693]\n",
      "loss: 0.250014  [ 7168/ 8693]\n",
      "loss: 0.250118  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.250173  [    0/ 8693]\n",
      "loss: 0.250010  [ 1024/ 8693]\n",
      "loss: 0.250110  [ 2048/ 8693]\n",
      "loss: 0.250515  [ 3072/ 8693]\n",
      "loss: 0.249829  [ 4096/ 8693]\n",
      "loss: 0.249669  [ 5120/ 8693]\n",
      "loss: 0.249805  [ 6144/ 8693]\n",
      "loss: 0.249968  [ 7168/ 8693]\n",
      "loss: 0.249691  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.249801  [    0/ 8693]\n",
      "loss: 0.249527  [ 1024/ 8693]\n",
      "loss: 0.249664  [ 2048/ 8693]\n",
      "loss: 0.250179  [ 3072/ 8693]\n",
      "loss: 0.249698  [ 4096/ 8693]\n",
      "loss: 0.250210  [ 5120/ 8693]\n",
      "loss: 0.249849  [ 6144/ 8693]\n",
      "loss: 0.249659  [ 7168/ 8693]\n",
      "loss: 0.250114  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.250165  [    0/ 8693]\n",
      "loss: 0.250420  [ 1024/ 8693]\n",
      "loss: 0.249870  [ 2048/ 8693]\n",
      "loss: 0.249866  [ 3072/ 8693]\n",
      "loss: 0.249822  [ 4096/ 8693]\n",
      "loss: 0.250020  [ 5120/ 8693]\n",
      "loss: 0.250086  [ 6144/ 8693]\n",
      "loss: 0.250002  [ 7168/ 8693]\n",
      "loss: 0.249934  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.250418  [    0/ 8693]\n",
      "loss: 0.249964  [ 1024/ 8693]\n",
      "loss: 0.250212  [ 2048/ 8693]\n",
      "loss: 0.250576  [ 3072/ 8693]\n",
      "loss: 0.249710  [ 4096/ 8693]\n",
      "loss: 0.249705  [ 5120/ 8693]\n",
      "loss: 0.249956  [ 6144/ 8693]\n",
      "loss: 0.249724  [ 7168/ 8693]\n",
      "loss: 0.250072  [ 8192/ 8693]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.249544  [    0/ 8693]\n",
      "loss: 0.249780  [ 1024/ 8693]\n",
      "loss: 0.249627  [ 2048/ 8693]\n",
      "loss: 0.250011  [ 3072/ 8693]\n",
      "loss: 0.250162  [ 4096/ 8693]\n",
      "loss: 0.250533  [ 5120/ 8693]\n",
      "loss: 0.250122  [ 6144/ 8693]\n",
      "loss: 0.249918  [ 7168/ 8693]\n",
      "loss: 0.250012  [ 8192/ 8693]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2339]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     train(train_dataloader, model, loss_fn, optimizer)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [2338]\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(dataloader, model)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      7\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 8\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     10\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [2335]\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 80\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_relu_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\functional.py:1442\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1440\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1442\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"data/test.csv\")\n",
    "training_data.fillna(0, inplace=True)\n",
    "seperated_columns = training_data['Cabin'].str.split('/', expand=True)\n",
    "training_data[['C1', 'C2', 'C3']] = seperated_columns\n",
    "seperated_columns = training_data['PassengerId'].str.split('_', expand=True)\n",
    "training_data[['PI1', 'PI2']] = seperated_columns.astype('int')\n",
    "seperated_columns = training_data['Name'].str.split(' ', expand=True)\n",
    "training_data['Last_name'] = seperated_columns[1]\n",
    "full_training_data = training_data\n",
    "training_data = full_training_data.drop(columns=['Cabin', 'Name'])\n",
    "cols = training_data.columns.tolist()\n",
    "print(cols)\n",
    "cols = cols[1:] + cols[:1]\n",
    "print(cols)\n",
    "training_data = training_data[cols]\n",
    "training_data.fillna(0, inplace=True)\n",
    "training_data = training_data.to_numpy()\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def replace_with_mapped_value_for_names(column_number, map_name, data_set):\n",
    "    def name_only(key):\n",
    "        value = last_name_map.get(key)\n",
    "        if value is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return value\n",
    "\n",
    "    data_set[:, column_number] = np.vectorize(name_only)(data_set[:, column_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "replace_with_mapped_value(0, planet_map, training_data)\n",
    "replace_with_mapped_value(1, TF_map, training_data)\n",
    "replace_with_mapped_value(2, destination_map, training_data)\n",
    "replace_with_mapped_value(4, TF_map, training_data)\n",
    "replace_with_mapped_value(10, cabin0, training_data)\n",
    "replace_with_mapped_value(12, cabin2, training_data)\n",
    "replace_with_mapped_value_for_names(15, last_name_map, training_data)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.load('model/ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dct = {v: k for k, v in TF_map.items()}\n",
    "dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for line in training_data:\n",
    "    current_line = torch.Tensor(line[:16].astype(float)).to(device)\n",
    "    pred = model(current_line)\n",
    "    result.append([line[16], dct[pred.argmax().item()]])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result = [['PassengerId', 'Transported']] + result\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"result.csv\", result, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
